{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Using neural networks to predict perovskite bandgaps*\n",
    "\n",
    "In this tutorial we will learn how to use neural networks from the [Keras](https://keras.io/) library to create a regression model to estimate perovskite bandgaps.\n",
    "\n",
    "You can find another example of neural network regression using Keras in the [TensorFlow Tutorials](https://nanohub.org/tools/tftutorials) nanoHUB tool.\n",
    "\n",
    "This tutorial uses Python, some familiarity with programming would be beneficial but is not required. Run each code cell in order by clicking \"Shift + Enter\". Feel free to modify the code to familiarize yourself with the workings on the code.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. Import libraries\n",
    "2. Getting data\n",
    "2. Processing and Organizing Data\n",
    "3. Creating and training the model\n",
    "4. Plotting\n",
    "\n",
    "**Get started:** Click \"Shift-Enter\" on the code cells to run! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries\n",
    "\n",
    "We first import the relevant libraries. These imports are over four cells:\n",
    "\n",
    "The first cell imports the Pandas and Numpy libraries that we will use to import and convert the data to appropriate formats for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featurization\n",
    "import cmcl\n",
    "from cmcl import Categories\n",
    "from spyglass.model_imaging import parityplot\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, normalize\n",
    "from sklearn.pipeline import make_pipeline as mkpipe\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell imports the Keras and Tensorflow libraries, which we use to construct the neural network. The third cell sets the random seed to ensure consistent results every time the notebook is run, an important step in reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell imports the pyplot module from the matplotlib library for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting a dataset\n",
    "\n",
    "We will follow the same steps used in [visualizations](./visualizations.ipynb) to load the data. We first load the CSV (comma separated value) file using the Pandas `read_csv` file. The `set_index` function uses a set of columns (passed as arguments) to index the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = pd.read_csv(\"./mannodi_data.csv\").set_index([\"index\", \"Formula\", \"sim_cell\"])\n",
    "lookup = pd.read_csv(\"./constituent_properties.csv\").set_index(\"Formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to convert the raw chemical formula into a numerical representation. This process is generally called featurization, and we will use the `cmcl` library to \"featurize\" the chemical formula. This library offers convenience functions such as `ft` that convert a raw string to a numerical representation, and `collect` that conveniently group this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = my.ft.comp() # compute numerical compostion vectors from strings\n",
    "mc = mc.collect.abx() # convenient site groupings for perovskites data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now group the data points (perovskites) into various categories (pure vs mixed) using the `Categories` class from `cmcl`. These categories are then assigned to the dataframes we loaded earlier, with the label mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixlog = mc.groupby(level=0, axis=1).count()\n",
    "mix = mixlog.pipe(Categories.logif, condition=lambda x: x>1, default=\"pure\", catstring=\"and\")\n",
    "mc = mc.assign(mix=mix).set_index(\"mix\", append=True)\n",
    "my = my.assign(mix=mix).set_index(\"mix\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixweight = pd.get_dummies(mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories assigned in the mix variable are now assigned numerical labels using the `OrdinalEncoder()` function from Scikit Learn. So the category pure is assigned the label 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixcat = pd.Series(OrdinalEncoder().fit_transform(mix.values.reshape(-1, 1)).reshape(-1),\n",
    "                     index=mc.index).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing and Organizing Data\n",
    "\n",
    "We now use the Scikit Learn Stratified Shuffle Split function to reserve 20% of the data as a test set, which we will not use in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=0)\n",
    "train_idx, test_idx = next(sss.split(mc, mixcat)) #stratify split by mix categories\n",
    "mc_tr, mc_ts = mc.iloc[train_idx], mc.iloc[test_idx]\n",
    "my_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]\n",
    "mixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now further divide the non-test set of the data into training and validation sets. The first 80% is reserved for training, and the remaining 20% is reserved as a validation set. The validation set is used to control model training, such as preventing overfitting. Before we define neural network and train the model, we need to replace the NaNs in the dataset with zeros, and normalize the inputs. We then convert to the Pandas dataframe to a Numpy array. Lastly, we use the `normalize()` function from Scikit Learn, which converts each row in the dataframe to a vector of length one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training and validation sets\n",
    "X = mc_tr\n",
    "Y = my_tr.PBE_bg_eV\n",
    "X = X.fillna(0) #replace nan with zero\n",
    "Y = Y.fillna(0)\n",
    "X = np.array(X, dtype='float32') #convert to numpy array\n",
    "Y = np.array(Y, dtype='float32')\n",
    "X = normalize(X) #normalize\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "idx = int(0.8*X.shape[0]) #Get a validation set\n",
    "Xtrain = X[:idx, :]\n",
    "Ytrain = Y[:idx, :]\n",
    "Xval = X[idx:, :]\n",
    "Yval = Y[idx:, :]\n",
    "\n",
    "#prepare testing set\n",
    "Xtest = mc_ts\n",
    "Ytest = my_ts.PBE_bg_eV\n",
    "Xtest = Xtest.fillna(0)\n",
    "Ytest = Ytest.fillna(0)\n",
    "Xtest = np.array(Xtest, dtype='float32')\n",
    "Ytest = np.array(Ytest, dtype='float32')\n",
    "Xtest = normalize(Xtest) \n",
    "Ytest = Ytest.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for consistency\n",
    "print (Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape, Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating and training the model\n",
    "\n",
    "For this regression, we will use a simple sequential neural network with two densely connected hidden layers, each with 100 neurons. The optimizer used will be the [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam). To learn more about the Adam Optimizer, click [here](https://climin.readthedocs.io/en/latest/adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential() #initialize a Sequential model\n",
    "model.add(keras.Input(shape=(14,))) #Add an input layer, the shape parameter tells how many inputs each data point will have\n",
    "model.add(layers.Dense(100, activation='tanh')) #Dense defines a fully connected layer, the argument specifies the number of neurons\n",
    "model.add(layers.Dense(100, activation='tanh')) #activation defines the activation function applied after each layer\n",
    "model.add(layers.Dense(1, activation='relu')) #Output layer can use a 'relu' activation since outputs are always positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    "- *Loss function:* This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n",
    "- *Optimizer:* This decides the optimization technique used to achieve a minimum for the loss function\n",
    "- *Epochs:* This decides how long to train the model. One epoch is defined as one iteration over the entire training set, where each iteration loops over all sample batches from the training set. Click [here](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9) to learn more about iterations, epochs and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3) # Initialize an Adam optimizer with a learning rate of 0.001\n",
    "model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError()) #Compile the model with the Adam optimizer and MSE loss\n",
    "EPOCHS = 100 #Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.fit()` function takes in the Numpy data we obtained earlier. This function automatically handles backpropogation and updating model weights. To learn more about backpropagation and how neural networks learn, you can watch the videos [here](https://www.youtube.com/watch?v=aircAruvnKk) or [here](https://www.youtube.com/watch?v=Ilg3gGewQ5U)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(Xtrain, Ytrain, epochs=EPOCHS, validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can check some of the [weights](https://en.wikipedia.org/wiki/Synaptic_weight) from the trained neural network. These weights, in a way, represent the relationship between inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "weights[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The history object contains the training and validation losses, which we can plot\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model.evaluate() function evaluates the model on the training, validation and testing datasets\n",
    "mse_train = model.evaluate(Xtrain, Ytrain)\n",
    "mse_val = model.evaluate(Xval, Yval)\n",
    "mse_test = model.evaluate(Xtest, Ytest)\n",
    "\n",
    "print(mse_train, mse_val, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the model into an h5 format training by using the `model.save()` function. This saved model can be reloaded using the `load_model()` function.\n",
    "\n",
    "This tool is running in a read only filesystem, so this code is just for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.save('./Models/nn_c_bg.h5')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_model = keras.models.load_model('./Models/nn_c_bg.h5')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plotting\n",
    "\n",
    "We now use pyplot from matplotlib to plot the \"Learning Curve\", which is a plot that shows the evolution of training and validation loss over epochs. We expect the training and validation losses to go down. More importantly, the validation loss helps monitor overfitting. Specifically, if the validation loss goes up, then we know that the model is overfitting. In Keras, this overfitting can be prevented using the `EarlyStopping()` functionality. LINK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss, label=\"train\")\n",
    "plt.plot(validation_loss, label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.predict()` makes predictions for different datasets. We will use this function to make predictions on the train, validation and test sets. We expect good predictions for the training and validation sets, but the predictions on the test sets are unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred_tr = model.predict(Xtrain)\n",
    "Y_pred_val = model.predict(Xval)\n",
    "Y_pred_test = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally plot a \"Parity Plot\" that measures the predictions compared to the true values. We see that the model does a reasonable job at predicting band gaps across train, validation, and test sets, indicating that the model has learnt the underlying correlations between composition and bandgap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ytrain, Y_pred_tr, 'ro')\n",
    "plt.plot(Yval, Y_pred_val, 'bo')\n",
    "plt.plot(Ytest, Y_pred_test, 'go')\n",
    "x = np.linspace(min(Ytrain), max(Ytrain), 1000)\n",
    "plt.plot(x, x, 'k-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the train test dataframes I've made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, data = parityplot(model, mc_tr.fillna(0), my_tr.PBE_bg_eV.to_frame(), aspect=1.0, hue=\"mix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrsicms",
   "language": "python",
   "name": "mrsicms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

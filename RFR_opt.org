#+TITLE: Optimizing RFR Model across multiple metrics
#+PROPERTY: header-args :session aikit :kernel aikit
* dependencies
#+begin_src jupyter-python :exports results :results raw drawer
  %load_ext autoreload
  %autoreload 2
#+end_src
  
#+begin_src jupyter-python :exports results :results raw drawer
  import sys
  sys.path.append("/home/panos/src/cmcl")
  sys.path.append("/home/panos/src/yogi")
  # featurization
  from cmcl.data.frame import *
  from cmcl.features.categories import Categories
  from yogi.model_selection.butler import summarize_HPO
  from yogi.metrics.index_aware_scoring import PandasScoreAdaptor as PSA
#+end_src

#+begin_src jupyter-python :exports results :results raw drawer
  ## accelerated ml pipeline ##
  from sklearnex import patch_sklearn
  patch_sklearn()
#+end_src
  
#+begin_src jupyter-python :exports results :results raw drawer
  # data tools
  import sqlite3
  import pandas as pd
  import numpy as np
  # feature engineering
  from sklearn.impute import SimpleImputer
  from sklearn.preprocessing import Normalizer, StandardScaler
  # predictors
  from sklearn.ensemble import RandomForestRegressor
  ## pipeline workflow
  from sklearn.pipeline import make_pipeline as mkpipe
  from sklearn.model_selection import train_test_split as tts
  from sklearn.model_selection import GridSearchCV as gsCV
  # model eval
  from sklearn.metrics import make_scorer, mean_squared_error, r2_score, explained_variance_score, max_error
  #visualization
  import matplotlib.pyplot as plt
  from sklearn import set_config
#+end_src
* load data
#+begin_src jupyter-python :exports results :results raw drawer
  sqlbase = """SELECT *
              FROM mannodi_base"""
  sqlref = """SELECT *
              FROM mannodi_ref_elprop"""
  sqlalmora = """SELECT *
                 FROM almora_agg"""
  with sqlite3.connect("/home/panos/src/cmcl/cmcl/db/perovskites.db") as conn:
      df = pd.read_sql(sqlbase, conn, index_col="index")
      lookup = pd.read_sql(sqlref, conn, index_col='index')
      almora = pd.read_sql(sqlalmora, conn, index_col='index')
#+end_src

* Clean Data
#+begin_src jupyter-python :exports results :results raw drawer
  lookup = lookup.set_index("Formula")
  df = df.set_index(["Formula", "sim_cell"], append=True)
#+end_src

** manual subset index + subset constituents
- drop formula with large lattice parameter difference between HSE and PBE (calculation to be rerun)
- large structural deformation identified by observing cubicity metric -- well outside of 5-10% spec?
#+begin_src jupyter-python :exports results :results raw drawer
  df = df.drop(index=["Rb0.375Cs0.625GeBr3", "RbGeBr1.125Cl1.875", "K0.75Cs0.25GeI3", "K8Sn8I9Cl15"], level=1)
  maincomp = df.ft.comp().iloc[:, :14:]
  empcomp = df.ft.comp().loc[:, ["FA", "MA", "Cs", "Pb", "Sn", "I", "Br", "Cl"]]
#+end_src

** auto subset index
#+begin_src jupyter-python :exports results :results raw drawer
  size = df.index.isin(["2x2x2"], level="sim_cell")
  #maincomp
  maincomp = maincomp.collect.abx()
  mcg = maincomp.groupby(level=0, axis=1).sum()
  mvB, mvX, mvA, = mcg.A.isin([1, 8]), mcg.B.isin([1, 8]), mcg.X.isin([3, 24])
  #emcomp
  empcomp = empcomp.collect.abx()
  ecg = empcomp.groupby(level=0, axis=1).sum()
  evB, evX, evA, = ecg.A.isin([1, 8]), ecg.B.isin([1, 8]), ecg.X.isin([3, 24])
  #subset indexes
  mfocus = size*mvB*mvA*mvX
  efocus = size*evB*evA*evX
#+end_src

** apply subsets
#+begin_src jupyter-python :exports results :results raw drawer
  mc = maincomp[mfocus]
  ec = empcomp[efocus]
  mys = df[mfocus]
  eys = df[efocus] #only 56 compounds
#+end_src

* Prepare test/train split and Pipelines of interest
** generate mix category
this helps with model evaluation for every subset in our data
#+begin_src jupyter-python :exports results :results raw drawer
  mixlog = mc.groupby(level=0, axis=1).count()
  mix = mixlog.pipe(Categories.logif, condition=lambda x: x>1, default="pure")
#+end_src

** construct pipeline + create test/train splits
Normalize each sampled composition prior to regression
#+begin_src jupyter-python :exports results :results raw drawer
  fillna = SimpleImputer(strategy="constant", fill_value=0.0)
  cpipeRFR = mkpipe(fillna, Normalizer(), RandomForestRegressor())
  ppipeRFR = mkpipe(StandardScaler(), RandomForestRegressor())
  mmix = mix[mfocus]
  emix = mix[efocus]
  mc_tr, mc_ts, my_tr, my_ts, mmix_tr, mmix_ts = tts(mc, mys, mmix,
                                                     train_size=0.8, random_state=0)
  ec_tr, ec_ts, ey_tr, ey_ts, emix_tr, emix_ts = tts(ec, eys, emix,
                                                     train_size=0.8, random_state=0)
#+end_src

** prepare subset scoring weights
#+begin_src jupyter-python :exports results :results raw drawer
  mixcat = pd.get_dummies(mmix_tr)
#+end_src

* Model BG using Combinations of Hyperparameters
** 1. construct original Hyper-parameter Space
#+begin_src jupyter-python :exports results :results raw drawer
  #"max_depth": [10, 20, 40],
  #"min_samples_split": [2, 5, 10]
  RFRgrid = [
      {'normalizer__norm': ['l1', 'l2', 'max'],
       'randomforestregressor__bootstrap': [True], #build each tree from sample
       'randomforestregressor__ccp_alpha': [0.0, 0.5, 1.0], #cost-complexity pruning
       'randomforestregressor__criterion': ['mse'], #['squared_error', 'poisson'], #update sklearn and try these
       'randomforestregressor__max_depth': [None], #investigate dept of constituent trees, limit
       'randomforestregressor__max_features': ['auto', 'sqrt', 3], #split after considering
       'randomforestregressor__max_leaf_nodes': [None], #investigate nodularity of trees
       'randomforestregressor__max_samples': [0.9, 0.6, 0.3], #frac to bag
       'randomforestregressor__min_impurity_decrease': [0.0],
       'randomforestregressor__min_impurity_split': [None],
       'randomforestregressor__min_samples_leaf': [1], #just sensible
       'randomforestregressor__min_samples_split': [2], #
       'randomforestregressor__min_weight_fraction_leaf': [0.0], #
       'randomforestregressor__n_estimators': [20, 50, 100],
       'randomforestregressor__n_jobs': [4], #parallelize exec
       'randomforestregressor__oob_score': [True], #use out-of-bag samples to validate (faster)
       'randomforestregressor__random_state': [None],
       'randomforestregressor__verbose': [0], 
       'randomforestregressor__warm_start': [False] #make a new forest every time (honest)
       },
      {'normalizer__norm': ['l1', 'l2', 'max'],
       'randomforestregressor__bootstrap': [False], #Build each tree from everything
       'randomforestregressor__ccp_alpha': [0.0, 0.5, 1.0], #cost-complexity pruning
       'randomforestregressor__criterion': ['mse'], #['squared_error', 'poisson'],
       'randomforestregressor__max_depth': [None], #investigate dept of constituent trees, limit
       'randomforestregressor__max_features': ['auto', 'sqrt', 3], #split after considering
       'randomforestregressor__max_leaf_nodes': [None], #investigate nodularity of trees
       'randomforestregressor__max_samples': [None], #"bag" everything
       'randomforestregressor__min_impurity_decrease': [0.0],
       'randomforestregressor__min_impurity_split': [None],
       'randomforestregressor__min_samples_leaf': [1], #just sensible
       'randomforestregressor__min_samples_split': [2], #
       'randomforestregressor__min_weight_fraction_leaf': [0.0], #
       'randomforestregressor__n_estimators': [20, 50, 100],
       'randomforestregressor__n_jobs': [4], #parallelize exec
       #oob score not available
       'randomforestregressor__random_state': [None],
       'randomforestregressor__verbose': [0], 
       'randomforestregressor__warm_start': [False] #make a new forest every time (honest)
       }
  ]
#+end_src

** 2. Composition model of PBE_BG
#+begin_src jupyter-python :exports results :results raw drawer :async yes
  site_mse = PSA(mean_squared_error).score
  RFRscoring = {'r2': make_scorer(r2_score),
                'ev': make_scorer(explained_variance_score),
                'maxerr': make_scorer(max_error, greater_is_better=False),
                'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),
                'A_rmse': make_scorer(site_mse, greater_is_better=False,
                                      squared=False, sample_weight=mixcat.A),
                'B_rmse': make_scorer(site_mse, greater_is_better=False,
                                      squared=False, sample_weight=mixcat.B),
                'X_rmse': make_scorer(site_mse, greater_is_better=False,
                                      squared=False, sample_weight=mixcat.X),
                'BandX_rmse': make_scorer(site_mse, greater_is_better=False,
                                          squared=False, sample_weight=mixcat.BandX),
                'Pure_rmse': make_scorer(mean_squared_error, greater_is_better=False,
                                         squared=False, sample_weight=mixcat.pure),}
  
  crfr = gsCV(estimator=cpipeRFR,
              param_grid=RFRgrid,
              cv=3, verbose=1, scoring=RFRscoring, refit="r2", return_train_score=True)
  crfr.fit(mc_tr, my_tr.PBE_bg_eV)
#+end_src
** 3. Determine next Grid Space to explore
#+begin_src jupyter-python :exports results :results raw drawer :async yes :pandoc org
  summary, next_grid = summarize_HPO(rfr, RFRgrid, topN=10, metric_weights=[1,1,1,1], strategy="oavg")
  summary
#+end_src

* BG -- Iteratively Optimize Hyperparameters
** 1. construct subsequent HP space
#+begin_src jupyter-python :exports results :results raw drawer :async yes :pandoc org
  RFRgrid = [
      {'normalizer__norm': ['l1'],
       'randomforestregressor__bootstrap': [True],
       'randomforestregressor__ccp_alpha': [0.0],
       'randomforestregressor__criterion': ['mse'],
       'randomforestregressor__max_depth': [None],
       'randomforestregressor__max_features': ['auto', 'sqrt'],
       'randomforestregressor__max_leaf_nodes': [None],
       'randomforestregressor__max_samples': [0.9],
       'randomforestregressor__min_impurity_decrease': [0.0],
       'randomforestregressor__min_impurity_split': [None],
       'randomforestregressor__min_samples_leaf': [1],
       'randomforestregressor__min_samples_split': [2],
       'randomforestregressor__min_weight_fraction_leaf': [0.0],
       'randomforestregressor__n_estimators': [50],
       'randomforestregressor__n_jobs': [4],
       'randomforestregressor__oob_score': [True],
       'randomforestregressor__random_state': [None],
       'randomforestregressor__verbose': [0],
       'randomforestregressor__warm_start': [False]},
      {'normalizer__norm': ['l1'],
       'randomforestregressor__bootstrap': [False],
       'randomforestregressor__ccp_alpha': [0.0],
       'randomforestregressor__criterion': ['mse'],
       'randomforestregressor__max_depth': [None],
       'randomforestregressor__max_features': ['auto', 'sqrt'],
       'randomforestregressor__max_leaf_nodes': [None],
       'randomforestregressor__max_samples': [None],
       'randomforestregressor__min_impurity_decrease': [0.0],
       'randomforestregressor__min_impurity_split': [None],
       'randomforestregressor__min_samples_leaf': [1],
       'randomforestregressor__min_samples_split': [2],
       'randomforestregressor__min_weight_fraction_leaf': [0.0],
       'randomforestregressor__n_estimators': [50],
       'randomforestregressor__n_jobs': [4],
       'randomforestregressor__random_state': [None],
       'randomforestregressor__verbose': [0],
       'randomforestregressor__warm_start': [False]}
  ]
#+end_src

* compute property descriptors
** create relational table
#+begin_src jupyter-python :exports results :results raw drawer
  mrel = mc.reset_index().melt(id_vars=maincomp.index.names).dropna(axis=0, subset="value")
  mrel = mrel.set_index(mc.index.names, append=False)
  erel = empcomp.reset_index().melt(id_vars=empcomp.index.names).dropna(axis=0, subset="value")
  erel = erel.set_index(empcomp.index.names, append=False)
#+end_src

** perform main join
#+begin_src jupyter-python :exports results :results raw drawer
  join = pd.merge(left=mrel, right=lookup, left_on="element", right_on="Formula")
  join = join.set_index(mrel.index)
  mainprop = join.groupby("site").apply(
      lambda df: df.groupby(level="Formula").apply(
          lambda df: pd.DataFrame(np.average(
              a=df.select_dtypes(include=np.number), axis=0, weights=df.value),
                                  index=df.select_dtypes(include=np.number).columns)))
  mainprop = mainprop.unstack(level="site").unstack(level=1)
  mainprop.columns=mainprop.columns.droplevel([0])
  mainprop = mainprop.drop(columns="value", level=1)
  mainprop = mainprop.reindex(index = maincomp.index.get_level_values("Formula"))
  mainprop.index=maincomp.index
#+end_src

** get empprop from mainprop
#+begin_src jupyter-python :exports results :results raw drawer
  empprop = mainprop.reindex(index=empcomp.index)
#+end_src
* Site-Averaged Properties Models
** Optimal Model on Properties                                     :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
StandardScale each prop feature prior to regression
#+begin_src jupyter-python :exports results :results raw drawer
ppipe = mkpipe(StandardScaler(), RandomForestRegressor())

X_train, X_test, y_train, y_test = train_test_split(maincomp, mys, test_size=0.2, random_state=0)

clf.fit(X_train, y_train)
print("model score: %.3f" % clf.score(X_test, y_test))
  cpipe = 
#+end_src

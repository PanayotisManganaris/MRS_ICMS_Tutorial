{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing RFR Models based on Perovskite Compositions\n",
    "======================================================\n",
    "\n",
    "**Author:** Panayotis Manganaris\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurization\n",
    "import cmcl\n",
    "from cmcl import Categories\n",
    "# multi-criterion model evaluation\n",
    "from yogi.model_selection import summarize_HPO\n",
    "from yogi.model_selection import pandas_validation_curve as pvc\n",
    "from yogi.metrics.pandas_scoring import PandasScoreAdaptor as PSA\n",
    "from yogi.metrics.pandas_scoring import batch_score\n",
    "# visualization convenience\n",
    "from spyglass.model_imaging import parityplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intel distribution provides accelerated ml algorithms. Run this cell before importing the algorithms. \"cmcl\" and \"yogi\" are our in-house modules for analyzing chemical compositions and enabling different nuts and bolts of ML algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tools\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# feature engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, Normalizer, StandardScaler\n",
    "# predictors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "## pipeline workflow\n",
    "from sklearn.pipeline import make_pipeline as mkpipe\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV as gsCV\n",
    "# model eval\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, explained_variance_score, max_error\n",
    "import joblib\n",
    "#visualization\n",
    "from sklearn import set_config\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# ignore all FutureWarnings -- handling coming in a future version of yogi\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Compute Composition Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Curated Subset of Mannodi Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = pd.read_csv(\"./mannodi_data.csv\").set_index([\"index\", \"Formula\", \"sim_cell\"])\n",
    "lookup = pd.read_csv(\"./constituent_properties.csv\").set_index(\"Formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Compostion Vectors using cmcl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, cmcl offers a convenience function for creating groups out of the column labels (groups = type of mixing in this dataset).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = my.ft.comp() # compute numerical compostion vectors from strings\n",
    "mc = mc.collect.abx() # convenient site groupings for perovskites data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate mix category and assign to index for future imaging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps with visualizing the final model performance by tracking the features through all the model development transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixlog = mc.groupby(level=0, axis=1).count()\n",
    "mix = mixlog.pipe(Categories.logif, condition=lambda x: x>1, default=\"pure\", catstring=\"and\")\n",
    "mc = mc.assign(mix=mix).set_index(\"mix\", append=True)\n",
    "my = my.assign(mix=mix).set_index(\"mix\", append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BG Using Composition Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Composition Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we use relies on a standard chain of data preprocessing steps. These steps are encapsulated in a single meta-estimator called a \"pipeline.\" The pipeline is instantiated with a set of default \"hyperparameters.\"\n",
    "\n",
    "Later, a hyperparameter optimization strategy based on exhaustive grid search will chose parameters that ensure the architecture performs as best it can for general high-accuracy prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "cpipe = mkpipe(fillna, Normalizer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Scheme\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The composition space is approximated by a set of discrete domains due the limited nature of the 2x2x2 supercells used to obtain this data. The pure domain is completely covered by 90 data points. The alloy domains are combinatorially large in the 14 dimensional component space under focus, and each has only been sparsely sampled.\n",
    "\n",
    "Our primary objective is to create a surrogate model of these domains that can be used for on-demand prediction, active learning, and inverse design.\n",
    "\n",
    "In order to target model generality, it will be necessary to score the model's performance with respect to it's predictions individually over each alloy domain and over the union of these domains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare subset scoring weights and ordinal group labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixweight = pd.get_dummies(mix)\n",
    "mixcat = pd.Series(OrdinalEncoder().fit_transform(mix.values.reshape(-1, 1)).reshape(-1),\n",
    "                     index=mc.index).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Scoring Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine metrics are used to monitor the fitness of the random forest model. Some monitor the data trend and spread:\n",
    "\n",
    "-   R<sup>2</sup>\n",
    "-   Explained Variance\n",
    "\n",
    "\n",
    "One monitors the largest breakdown in accuracy:\n",
    "\n",
    "-   Max Error\n",
    "\n",
    "\n",
    "The remaining six group-wise RMSE metrics monitor the accuracy for each type of data point:\n",
    "\n",
    "-   total RMSE\n",
    "-   A-site RMSE\n",
    "-   B-site RMSE\n",
    "-   X-site RMSE\n",
    "-   XandB-site RMSE\n",
    "-   pure RMSE\n",
    "\n",
    "\n",
    "The PandasScoreAdaptor (PSA) ensures the prediction losses are weighted correctly when scoring as long as both the targets and the sample weights passed to the estimators are always pandas objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_mse = PSA(mean_squared_error).score\n",
    "scorings = {'r2': make_scorer(r2_score),\n",
    "            'ev': make_scorer(explained_variance_score),\n",
    "            'maxerr': make_scorer(max_error, greater_is_better=False),\n",
    "            'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n",
    "            'A_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.A),\n",
    "            'B_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.B),\n",
    "            'X_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.X),\n",
    "            'BandX_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                      squared=False, sample_weight=mixweight.BandX),\n",
    "            'Pure_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                     squared=False, sample_weight=mixweight.pure),}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dedicated Test Train Split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dedicated test-train split is made. This split preserves the proportion of each alloy group in the test and train partitions, which helps with the final model evaluation. \n",
    "\n",
    "-   All decisions about model optimization will be made using only the dedicated training partition.\n",
    "-   The test partition will be reserved until a final model pipeline is parametrized and fit.\n",
    "-   The predictions made on the test partition will either confirm or deny the model's ability to work outside of the training domain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)\n",
    "train_idx, test_idx = next(sss.split(mc, mixcat)) #stratify split by mix categories\n",
    "mc_tr, mc_ts = mc.iloc[train_idx], mc.iloc[test_idx]\n",
    "my_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]\n",
    "mixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves &#x2013; Using Deterministically Random Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation within the training set will be the only way of checking the generality of models, aside from predictions on the test set. The first step is to obtain an understanding of how much data is needed for training that can be suitably generalized.\n",
    "\n",
    "10-fold cross-validation yields 10 sample scores at each partition size. This means 90% of the training set is used for actual training and the remaining 10% is used for validation.\n",
    "\n",
    "The shuffling is seeded with a deterministic random state to ensure scores are comparable across partition size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('multiprocessing'):\n",
    "  LC = pvc(learning_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n",
    "           train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc, scoring=scorings)\n",
    "  LC = LC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the error metrics are negated so that, consistently with\n",
    "the R<sup>2</sup> and ev scores, the greater the number, the better the model\n",
    "performs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.FacetGrid(LC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\n",
    "p.map(sns.lineplot, \"train_sizes\", \"value\")\n",
    "p.add_legend()\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest's validation scores continue to rise as the partition size grows\n",
    "\n",
    "-   Random Forest generality increases with more exposure\n",
    "-   Equivalently, an insufficiently experienced random forest is biased towards what it has seen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Generality Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four main alloy classes are roughly equally represented in the dataset. It would be interesting to see if a model trained on only three of them performs well on the fourth.\n",
    "\n",
    "There are four main groups, so we use four groupwise partitions. The tiny \"BandX\" group is mixed in with the others -- it's name is not printed by default. The splitter uses the mixcat ordinal series to ensure the index splits are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is defined to streamline this test and ensure it cannot contaminate the estimator to be optimized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generality(estimator, X_tr, y_tr, groups_tr, X_ts, y_ts, groups_ts):\n",
    "    estimator = clone(estimator) #unfitted, cloned params\n",
    "    gentpl = gkf.split(X_tr, y_tr, groups=groups_tr), gkf.split(X_ts, y_ts, groups=groups_ts)\n",
    "    #train and test index generators, in order\n",
    "    val_scores = []\n",
    "    tst_scores = []\n",
    "    for train_idx, val_idx, _, tst_idx in [sum(gengroup, ()) for gengroup in zip(*gentpl)]:\n",
    "        tr_val_group_names = groups_tr.iloc[val_idx].index.get_level_values(\"mix\").unique()\n",
    "        ts_group_names = groups_ts.iloc[tst_idx].index.get_level_values(\"mix\").unique()\n",
    "        #fit to tr part\n",
    "        estimator.fit(X_tr.iloc[train_idx], y_tr.iloc[train_idx])\n",
    "        #get val and test scores\n",
    "        tr_val_score_series = pd.Series(batch_score(estimator, X_tr.iloc[val_idx], y_tr.iloc[val_idx], **scorings))\n",
    "        tr_val_score_series.name=tr_val_group_names[0]#\"_&_\".join(tr_val_group_names) #delete the indexed list and uncomment the join operation to see where BandX was put\n",
    "        ts_score_series = pd.Series(batch_score(estimator, X_ts.iloc[tst_idx], y_ts.iloc[tst_idx], **scorings))\n",
    "        ts_score_series.name=ts_group_names[0]#\"_&_\".join(ts_group_names)\n",
    "        val_scores.append(tr_val_score_series)\n",
    "        tst_scores.append(ts_score_series)\n",
    "    tr_val_scores = pd.concat(val_scores, axis=1).assign(partition=\"validation\")\n",
    "    ts_scores = pd.concat(tst_scores, axis=1).assign(partition=\"test\")\n",
    "    group_scores = pd.concat([tr_val_scores, ts_scores]).round(5).drop_duplicates(keep=\"first\")\n",
    "    return group_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   There are substantial errors in all the groups\n",
    "    -   in the test partition, the model utterly fails in extrapolating the bandgaps of B-mixed alloys\n",
    "    -   in the train partition, it's still not good\n",
    "    -   however, extrapolating the other partitions is relatively much better\n",
    "-   The B partition is mostly representative of the others\n",
    "    -   w.r.t the underlying function as identified by this specific model architecture\n",
    "    -   different architectures can be inclined to learn different characteristics of a function\n",
    "\n",
    "**Note:** batch_score defaults to unweighted scoring if the sum of weights\n",
    "in the given sample equal zero, that is why most of the group-wise rmse\n",
    "scores are the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Hyper-parameters for Composition Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define first level of Hyperparameter search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells archive the optimization process, **running them all is not necessary**. They are very time consuming. The results of the search are saved in the notebook.\n",
    "\n",
    "**Note:** The Hyperparameter Optimization has been abridged for time. Only the largest grid-search and the final grid-search are shown. In practice, it is best to iterate through the search space as many times as are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct original Hyper-parameter Space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1', 'l2', 'max'],\n",
    "     'randomforestregressor__bootstrap': [True], #build each tree from sample\n",
    "     'randomforestregressor__ccp_alpha': [0.0, 0.002], #cost-complexity pruning\n",
    "     'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'poisson'], #variance reductions vs deviance reduction\n",
    "     #'randomforestregressor__maxBins:': [256], #this is an artifact of the intel algorithm, it is not a valid key.\n",
    "     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n",
    "     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n",
    "     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n",
    "     'randomforestregressor__max_samples': [0.9, 0.6, 0.3], #frac to bag\n",
    "     'randomforestregressor__minBinSize': [1],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #0.3 corresponds to the onset of aggressive ccp\n",
    "     'randomforestregressor__min_samples_leaf': [1], #just sensible\n",
    "     'randomforestregressor__min_samples_split': [2, 5], #\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n",
    "     'randomforestregressor__n_estimators': [20, 50, 100],\n",
    "     'randomforestregressor__n_jobs': [4], #parallelize exec\n",
    "     'randomforestregressor__oob_score': [True], #use out-of-bag samples to validate (faster)\n",
    "     'randomforestregressor__random_state': [None], #do not touch\n",
    "     'randomforestregressor__verbose': [0], \n",
    "     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n",
    "     },\n",
    "    {'normalizer__norm': ['l1', 'l2', 'max'],\n",
    "     'randomforestregressor__bootstrap': [False], #Build each tree from everything\n",
    "     'randomforestregressor__ccp_alpha': [0.0, 0.002], #cost-complexity pruning\n",
    "     'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'poisson'], #variance reductions vs deviance reduction\n",
    "     #'randomforestregressor__maxBins:': [256],\n",
    "     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n",
    "     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n",
    "     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n",
    "     'randomforestregressor__max_samples': [None], #\"bag\" everything\n",
    "     'randomforestregressor__minBinSize': [1],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #\n",
    "     'randomforestregressor__min_samples_leaf': [1], #just sensible\n",
    "     'randomforestregressor__min_samples_split': [2, 5], #\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n",
    "     'randomforestregressor__n_estimators': [20, 50, 100],\n",
    "     'randomforestregressor__n_jobs': [4], #parallelize exec\n",
    "     #oob score not available\n",
    "     'randomforestregressor__random_state': [None], #do not touch\n",
    "     'randomforestregressor__verbose': [0], \n",
    "     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition model of PBE_bg_eV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initially, only 3 fold validation is used to save on computation time, this is a reasonable compromise on training generality according to the learning curves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgs = gsCV(estimator=cpipe,\n",
    "            param_grid=grid,\n",
    "            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n",
    "\n",
    "with joblib.parallel_backend('multiprocessing'):\n",
    "    cgs.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 10368 candidates, totalling 31104 fits &#x2013; about 45 minutes to fit with acceleration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine next Grid Space to explore\n",
    "\n",
    "The cell below provides a HP search grid and applies weights of importance to each of the 9 error metrics (B_rmse is given a weight \"2\" to improve B-site mixed predictions further)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly Equal Weights In Summary:\n",
    "\n",
    "| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| normalizer_<sub>norm</sub>|[l1, l2, max]|[l1, l2, max]|0.853155|0.853155|[19.07, 0.07, 10.74]|[19.07, 0.07, 10.74]|[l1, max]|[l1, max]|\n",
    "| bootstrap|[True]|[False]|0.142457|0.289120|[26.18]|[3.69]|[True]|[False]|\n",
    "| ccp<sub>alpha</sub>|[0.0, 0.002]|[0.0, 0.002]|0.485723|0.485723|[27.33, 2.54]|[27.33, 2.54]|[0.0]|[0.0]|\n",
    "| criterion|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|0.687787|0.687787|[18.12, 11.75, 0.0]|[18.12, 11.75, 0.0]|[squared<sub>error</sub>, absolute<sub>error</sub>]|[squared<sub>error</sub>, absolute<sub>error</sub>]|\n",
    "| max<sub>depth</sub>|[25, 20]|[25, 20]|0.683604|0.683604|[14.16, 15.72]|[14.16, 15.72]|[20]|[20]|\n",
    "| max<sub>features</sub>|[auto, 3, 5]|[auto, 3, 5]|0.759206|0.759206|[25.04, 0.06, 4.78]|[25.04, 0.06, 4.78]|[auto]|[auto]|\n",
    "| max<sub>leaf</sub><sub>nodes</sub>|[750, 800]|[750, 800]|0.692553|0.692553|[14.88, 15.0]|[14.88, 15.0]|[800]|[800]|\n",
    "| max<sub>samples</sub>|[0.9, 0.6, 0.3]|[None]|0.812818|0.289120|[21.28, 4.82, 0.09]|[0.16]|[0.9]|[None]|\n",
    "| minBinSize|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n",
    "| min<sub>impurity</sub><sub>decrease</sub>|[0.0, 0.3]|[0.0, 0.3]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n",
    "| min<sub>samples</sub><sub>split</sub>|[2, 5]|[2, 5]|0.485723|0.485723|[26.1, 3.77]|[26.1, 3.77]|[2]|[2]|\n",
    "| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.083522|1.083522|[7.95, 8.22, 13.7]|[7.95, 8.22, 13.7]|[100]|[100]|\n",
    "| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n",
    "| oob<sub>score</sub>|[True]|NaN|0.142457|NaN|[0.84]|NaN|[True]|NaN|\n",
    "| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n",
    "| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n",
    "| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n",
    "\n",
    "-   l1 normalization is best\n",
    "-   bootstrapping the regressor is much more performant\n",
    "    -   90% sampling is best (rfr improves with more exposure, makes sense)\n",
    "    -   notice: bootstrap sampling appears to rank only slightly more\n",
    "        frequently in the top ten than no-bootstrap, but has much higher\n",
    "        scores. suggesting it also dominates the highest ranks in general.\n",
    "-   max normalization also does well, but not as well\n",
    "-   squared error does best\n",
    "-   absolute<sub>error</sub> (more expensive) is less susceptible to compromising on extremes, but appears mostly unfavorable\n",
    "-   limiting tree depth slightly better than not limiting it\n",
    "-   growth on all features better than growth on few features. larger axis limits yet to be explored\n",
    "-   unlimited nodes marginally better than limited nodes\n",
    "-   impurity decrease threshold is ineffective\n",
    "-   unlimited split granularity better than limited granularity\n",
    "-   generally, more estimators outperform fewer\n",
    "\n",
    "Favorably Weighting B scores:\n",
    "\n",
    "| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| normalizer_<sub>norm</sub>|[l1, l2, max]|[l1, l2, max]|0.853155|0.853155|[21.94, 0.07, 11.25]|[21.94, 0.07, 11.25]|[l1, max]|[l1, max]|\n",
    "| bootstrap|[True]|[False]|0.142457|0.289120|[29.47]|[3.79]|[True]|[False]|\n",
    "| ccp<sub>alpha</sub>|[0.0, 0.002]|[0.0, 0.002]|0.485723|0.485723|[30.68, 2.58]|[30.68, 2.58]|[0.0]|[0.0]|\n",
    "| criterion|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|0.687787|0.687787|[21.39, 11.87, 0.0]|[21.39, 11.87, 0.0]|[squared<sub>error</sub>, absolute<sub>error</sub>]|[squared<sub>error</sub>, absolute<sub>error</sub>]|\n",
    "| max<sub>depth</sub>|[25, 20]|[25, 20]|0.683604|0.683604|[16.54, 16.72]|[16.54, 16.72]|[20]|[20]|\n",
    "| max<sub>features</sub>|[auto, 3, 5]|[auto, 3, 5]|0.759206|0.759206|[27.43, 0.06, 5.77]|[27.43, 0.06, 5.77]|[auto]|[auto]|\n",
    "| max<sub>leaf</sub><sub>nodes</sub>|[750, 800]|[750, 800]|0.692553|0.692553|[17.34, 15.92]|[17.34, 15.92]|[750]|[750]|\n",
    "| max<sub>samples</sub>|[0.9, 0.6, 0.3]|[None]|0.812818|0.289120|[22.68, 6.69, 0.09]|[0.16]|[0.9]|[None]|\n",
    "| minBinSize|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n",
    "| min<sub>impurity</sub><sub>decrease</sub>|[0.0, 0.3]|[0.0, 0.3]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n",
    "| min<sub>samples</sub><sub>split</sub>|[2, 5]|[2, 5]|0.485723|0.485723|[28.39, 4.87]|[28.39, 4.87]|[2]|[2]|\n",
    "| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.083522|1.083522|[9.59, 8.5, 15.18]|[9.59, 8.5, 15.18]|[100]|[100]|\n",
    "| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n",
    "| oob<sub>score</sub>|[True]|NaN|0.142457|NaN|[0.84]|NaN|[True]|NaN|\n",
    "| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n",
    "| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n",
    "| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n",
    "\n",
    "-   gap between limited and unlimited tree depth closes slightly\n",
    "-   limited leaf nodes becomes more favorable than unlimited leaf nodes &#x2013; reversal!\n",
    "-   20 estimators actually ranks much higher. less averaging => more bias helps B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipping Iterations...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x2013; Iteratively Optimize Hyperparameters (final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct subsequent HP space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the limited tree domains, without interference from n_estimators, various criterion are explored in detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [True],\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'poisson'],\n",
    "     'randomforestregressor__max_depth': [15, 20],\n",
    "     'randomforestregressor__max_features': ['auto'],\n",
    "     'randomforestregressor__max_leaf_nodes': [600, 700, 800],\n",
    "     'randomforestregressor__max_samples': [0.9],\n",
    "     'randomforestregressor__minBinSize': [1],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [100], #compromise in anticipation of possible overfitting\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__oob_score': [True],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition model of PBE_bg_eV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgs = gsCV(estimator=cpipe,\n",
    "            param_grid=grid,\n",
    "            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n",
    "\n",
    "with joblib.parallel_backend('multiprocessing'):\n",
    "    cgs.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine next Grid Space to explore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Summary:\n",
    "\n",
    "| |space<sub>0</sub>|entropy<sub>0</sub>|scores<sub>0</sub>|next<sub>0</sub>|\n",
    "|---|---|---|---|---|\n",
    "| normalizer_<sub>norm</sub>|[l1]|-0.000000|NaN|[l1]|\n",
    "| bootstrap|[True]|-0.000000|NaN|[True]|\n",
    "| ccp<sub>alpha</sub>|[0.0]|-0.000000|NaN|[0.0]|\n",
    "| criterion|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|1.054920|[18.27, 14.65, 2.16]|[squared<sub>error</sub>, absolute<sub>error</sub>]|\n",
    "| max<sub>depth</sub>|[15, 20]|0.673012|[10.35, 24.73]|[20]|\n",
    "| max<sub>features</sub>|[auto]|-0.000000|NaN|[auto]|\n",
    "| max<sub>leaf</sub><sub>nodes</sub>|[600, 700, 800]|1.098612|[8.45, 12.28, 14.35]|[700, 800]|\n",
    "| max<sub>samples</sub>|[0.9]|-0.000000|NaN|[0.9]|\n",
    "| minBinSize|[1]|-0.000000|NaN|[1]|\n",
    "| min<sub>impurity</sub><sub>decrease</sub>|[0.0]|-0.000000|NaN|[0.0]|\n",
    "| min<sub>samples</sub><sub>leaf</sub>|[1]|-0.000000|NaN|[1]|\n",
    "| min<sub>samples</sub><sub>split</sub>|[2]|-0.000000|NaN|[2]|\n",
    "| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|-0.000000|NaN|[0.0]|\n",
    "| n<sub>estimators</sub>|[100]|-0.000000|NaN|[100]|\n",
    "| n<sub>jobs</sub>|[4]|-0.000000|NaN|[4]|\n",
    "| oob<sub>score</sub>|[True]|-0.000000|NaN|[True]|\n",
    "| random<sub>state</sub>|[None]|-0.000000|NaN|[None]|\n",
    "| verbose|[0]|-0.000000|NaN|[0]|\n",
    "| warm<sub>start</sub>|[False]|-0.000000|NaN|[False]|\n",
    "\n",
    "-   squared error and absolute error perform similarly in this area\n",
    "    -   absolute error is more expensive to use,\n",
    "    -   choosing square error is justifiable for practicality\n",
    "-   gentle limits are preferred to hard limits\n",
    "\n",
    "B score weighting makes no difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform sensitivity analysis for n_estimators in optimal subspace\n",
    "\n",
    "Again, as many parameter sensitivity analysis should be performed as needed. For this architecture n_estimators, max_depth, and max_leaf_nodes are all potentially sensitive parameters as seen in the HPO iterations. Here only n_estimators is studied for time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot a validation curve over n_estimators when considering reasonable\n",
    "tree parameters.\n",
    "\n",
    "The model is probably more sensitive to n_estimators.\n",
    "\n",
    "Adjusting the ensemble size to fit the tree optimizations is not as\n",
    "much a sensible use of the RFR architecture's strengths, so it is used\n",
    "first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct subsequent HP space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [True],\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['squared_error'],\n",
    "     'randomforestregressor__max_depth': [20],\n",
    "     'randomforestregressor__max_features': ['auto'],\n",
    "     'randomforestregressor__max_leaf_nodes': [700],\n",
    "     'randomforestregressor__max_samples': [0.9],\n",
    "     'randomforestregressor__minBinSize': [1],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [100],\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__oob_score': [True],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_estimators validation scan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the 4 fold cross validation established by the LC analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('multiprocessing'):\n",
    "  VC = pvc(validation_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n",
    "           param_name='randomforestregressor__n_estimators', param_range=np.linspace(50, 150, 15).astype(int), cv=4, scoring=scorings)\n",
    "  VC = VC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.FacetGrid(VC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\n",
    "p.map(sns.lineplot, 'randomforestregressor__n_estimators', \"value\")\n",
    "p.add_legend()\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Model scores in this parameter subspace appear to be insensitive to n_stimators. This is a pleasant surprise.\n",
    "-   maxerror is consistently better between 110 and 120 estimators -- this is chosen going forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [True],\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['squared_error'],\n",
    "     'randomforestregressor__max_depth': [22],\n",
    "     'randomforestregressor__max_features': ['auto'],\n",
    "     'randomforestregressor__max_leaf_nodes': [660],\n",
    "     'randomforestregressor__max_samples': [0.9],\n",
    "     'randomforestregressor__minBinSize': [1],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [115],\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__oob_score': [True],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Final Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})\n",
    "cpipe.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change between tr and ts suffixes to see test vs train pairity plot\n",
    "p, data = parityplot(cpipe, mc_ts, my_ts.PBE_bg_eV.to_frame(), aspect=1.0, hue=\"mix\")\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change between tr and ts suffixes to see test vs train scores -- both are good\n",
    "pd.Series(batch_score(cpipe, mc_ts, my_ts.PBE_bg_eV, **scorings)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFR designed here scores consistently well with the test data,\n",
    "However, parity visual could indicate nonetheless show possible\n",
    "over fitting to the training data if the spread is not consistent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Generality Measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The post-optimization RFR generalizes across all alloy types much\n",
    "better than the default parameters, but there are clearly limitations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting/Importing Trained Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model for distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never load joblib/pickle files that you do not trust, they can execute\n",
    "arbitrary code on your computer.\n",
    "\n",
    "This tool is running on a read only filesystem, so this code is just for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joblib.dump(cpipe, \"./Models/rfr_c_opt.joblib\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cpipe = joblib.load(\"./Models/rfr_c_opt.joblib\")`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrsicms",
   "language": "python",
   "name": "mrsicms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

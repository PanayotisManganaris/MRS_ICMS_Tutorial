{"cells":[{"cell_type":"markdown","metadata":{},"source":"Optimizing RFR Models based on Perovskite Compositions\n======================================================\n\n**Author:** Panayotis Manganaris\n\n"},{"cell_type":"markdown","metadata":{},"source":["## dependencies\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# featurization\nimport cmcl\nfrom cmcl import Categories\n# multi-criterion model evaluation\nfrom yogi.model_selection import summarize_HPO\nfrom yogi.model_selection import pandas_validation_curve as pvc\nfrom yogi.metrics.pandas_scoring import PandasScoreAdaptor as PSA\nfrom yogi.metrics.pandas_scoring import batch_score\n# visualization convenience\nfrom spyglass.model_imaging import parityplot"]},{"cell_type":"markdown","metadata":{},"source":["The Intel distribution provides accelerated ml algorithms. Run this\ncell before importing the algorithms\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearnex import patch_sklearn\npatch_sklearn()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# data tools\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n# feature engineering\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, Normalizer, StandardScaler\n# predictors\nfrom sklearn.ensemble import RandomForestRegressor\n## pipeline workflow\nfrom sklearn.pipeline import make_pipeline as mkpipe\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV as gsCV\n# model eval\nfrom sklearn.metrics import make_scorer, mean_squared_error, r2_score, explained_variance_score, max_error\n#visualization\nfrom sklearn import set_config\nimport matplotlib.pyplot as plt\nimport seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data and Compute Composition Vectors\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Load Curated Subset of Mannodi Data\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mannodi = pd.read_csv(\"./mannodi_data.csv\").set_index([\"index\", \"Formula\", \"sim_cell\"])\nlookup = pd.read_csv(\"./constituent_properties.csv\").set_index(\"Formula\")"]},{"cell_type":"markdown","metadata":{},"source":["### Compute Compostion Vectors using cmcl\n\n"]},{"cell_type":"markdown","metadata":{},"source":["additionally, cmcl offers a convenience function for creating groups\nout of the column labels\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mc = mannodi.ft.comp() # compute numerical compostion vectors from strings\nmc = mc.collect.abx() # convenient site groupings for perovskites data"]},{"cell_type":"markdown","metadata":{},"source":["### generate mix category and assign to index for future imaging\n\n"]},{"cell_type":"markdown","metadata":{},"source":["this helps with visualizing the final model performance by tracking\nthe features through all the model development transformations\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mixlog = mc.groupby(level=0, axis=1).count()\nmix = mixlog.pipe(Categories.logif, condition=lambda x: x>1, default=\"pure\", catstring=\"and\")\nmc = mc.assign(mix=mix).set_index(\"mix\", append=True)\nmy = my.assign(mix=mix).set_index(\"mix\", append=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Model BG Using Composition Vectors\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Make Composition Pipeline\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The model we use relies on a standard chain of data preprocessing\nsteps. These steps will be encapsulated in a single meta-estimator\ncalled a \"pipeline.\"\n\nThis pipeline is constructed to handle the expected form of the 14\ndimensional composition space our data covers. The regression is only\nperformed after an imputer treats N/A constituent quantities as zero\nand a normalizer transformer ensures a bounded domain. The regressor\nitself is initialized now using only default \"hyperparameters.\"\n\nHyperparameters control aspects of the regression algorithm's behavior\nthat are not learned from the data. Later, a hyperparameter\noptimization strategy based on exhaustive grid search will chose\nparameters that ensure the architecture performs as best it can for\nthe purposes of inverse design.\n\nThe process for determining how suitable a model is for this purpose\nis discussed next.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fillna = SimpleImputer(strategy=\"constant\", fill_value=0.0)\ncpipe = mkpipe(fillna, Normalizer(), RandomForestRegressor())"]},{"cell_type":"markdown","metadata":{},"source":["### Scoring Scheme\n\n"]},{"cell_type":"markdown","metadata":{},"source":["This dataset posits a challenge to successful modeling. We seek to\ncreate a regressor which generalizes well to perovskites of any alloy\ncharacter. As seen in the visualizations, the dataset contains mainly\npure, A, B and X-site alloyed perovskites. Some of which are defined\nmostly on axes with substantially differing statistical profiles.\nThereby inviting a model to fit to artifacts of the sample\ndistributions rather than physically meaningful information. Starting\nnow, great care will be taken to mitigate this.\n\nThe composition space is approximated by a set of discrete domains due\nthe limited nature of the 2x2x2 supercells used to obtain this\ndata. The pure domain is completely covered by 90 data points. The\nalloy domains are combinatorial large in the 14 dimensional component\nspace under focus, and each has only been sparsely sampled.\n\nRecall, our primary objective is to create a surrogate model of these\ndomains that can inform future computations needed to exhaustively\nexplore it.\n\nIn order to validate that our model generalizes well, it will be\nnecessary to score the model's performance with respect to it's\npredictions individually over each alloy domain simultaneously with\nit's prediction over the union of these domains.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### prepare subset scoring weights and ordinal group labels\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mixweight = pd.get_dummies(mix)\nmixcat = pd.Series(OrdinalEncoder().fit_transform(mix.values.reshape(-1, 1)).reshape(-1),\n                     index=mc.index).astype(int)"]},{"cell_type":"markdown","metadata":{},"source":["#### Define Scoring Metrics\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Nine metrics are used to monitor the fitness of the random forest model.\n\n-   R<sup>2</sup> and Explained Variance scores keep track of the regression's ability to capture the data trend and spread\n-   Max Error helps to keep track the largest breakdown in accuracy.\n-   Additionally six group-wise RMSE metrics are kept.\n    -   total RMSE\n    -   A-site RMSE\n    -   B-site RMSE\n    -   X-site RMSE\n    -   XandB-site RMSE\n    -   pure RMSE\n\nthe PandasScoreAdaptor (PSA) ensures the prediction losses are\nweighted correctly when scoring so long as both the targets and the\nsample weights passed to the estimators are always pandas objects\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["site_mse = PSA(mean_squared_error).score\nscorings = {'r2': make_scorer(r2_score),\n            'ev': make_scorer(explained_variance_score),\n            'maxerr': make_scorer(max_error, greater_is_better=False),\n            'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n            'A_rmse': make_scorer(site_mse, greater_is_better=False,\n                                  squared=False, sample_weight=mixweight.A),\n            'B_rmse': make_scorer(site_mse, greater_is_better=False,\n                                  squared=False, sample_weight=mixweight.B),\n            'X_rmse': make_scorer(site_mse, greater_is_better=False,\n                                  squared=False, sample_weight=mixweight.X),\n            'BandX_rmse': make_scorer(site_mse, greater_is_better=False,\n                                      squared=False, sample_weight=mixweight.BandX),\n            'Pure_rmse': make_scorer(site_mse, greater_is_better=False,\n                                     squared=False, sample_weight=mixweight.pure),}"]},{"cell_type":"markdown","metadata":{},"source":["### Make Dedicated Test Train Split\n\n"]},{"cell_type":"markdown","metadata":{},"source":["At this point, a dedicated test-train split is made. This split\npreserves the proportion of each alloy group in the test and train\npartitions, which helps with the final model evaluation.\n\nBeyond this point, all decisions about model optimization will be made\nusing only the dedicated training partition. The test partition will\nbe reserved until a final model pipeline is parametrized and\nfit. Then, the predictions made on the test partition will either\nconfirm or deny the model's ability to work outside of the training\ndomain. The model that ultimately scores best in this respect will be\nused to perform inverse design.\n\nThe partition is essentially arbitrary. In this case, just enough test\ndata is held aside to represent at least one sample of each alloy\ntype in the final tests.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)\ntrain_idx, test_idx = next(sss.split(mc, mixcat)) #stratify split by mix categories\nmc_tr, mc_ts = mc.iloc[train_idx], mc.iloc[test_idx]\nmy_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]\nmixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]"]},{"cell_type":"markdown","metadata":{},"source":["### Learning Curves &#x2013; Using Deterministically Random Cross Validation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Cross-validation within the training set will be the only way of\nchecking the generality of any given model instance. So, first, it is\nnecessary to have some understanding of how much data is needed to\ntrain the model in order for it to have a chance of generalizing.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)"]},{"cell_type":"markdown","metadata":{},"source":["Here a splitter is set to produce a 10 cross validation folds on\nrandomly shuffled data indices. This means that 9 examples of a fit\nmodel will be will be produced for each partition size to be tested.\nThe 10th fold will likewise be checked 9 times for each corresponding\ntraining. So, effectively at each of the partition sizes tested, the\nmodel is given 9 chances to perform after seeing 90% of the data. This\nis generous. If the model fails to generalize under these conditions\nat some partition size, it is probably a lost cause.\n\nNotice, the splitter will also perform internal shuffling of the data\nindices. (The shuffling occurs before the splitting in each\ncase). This helps to prevent the model from training on the ordered\ngroupings in the dataset. For these learning curves, the splitter is\ninitialized with a deterministic random state. This ensures that each\nfold generated at each partition size in the learning curve is not\nonly comparable to the others in its split, but to those in\nneighboring partition sizes as well.\n\nIn the future, when performing hyperparameter optimization, the cv\nsplitters used will use the global random state. This will help to\nensure that the shuffle seen by the myriad fit calls will not be\ntreated itself as a hyperparameter to be optimized. The eventual final\nmodel should therefore be robust to the order of data.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["LC = pvc(learning_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n         train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc, scoring=scorings)\nLC = LC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["Notice that the error metrics are negated so that, consistently with\nthe R<sup>2</sup> and ev scores, the greater the number, the better the model\nperforms.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p = sns.FacetGrid(LC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\np.map(sns.lineplot, \"train_sizes\", \"value\")\np.add_legend()"]},{"cell_type":"markdown","metadata":{},"source":["It appears that 3-4 fold cross-validation is sufficient (training with\n300/400 or 260/400 points, validating with the compliment)\n\nThe learning curve indicates a few things about the bias and variance\nof the model in question. The Random Forest's validation scores\ncontinue to rise as the partition size grows. This implies that the\nmodel's generality likely increases with more exposure. Equivalently,\nan insufficiently experienced random forest tends to be biased towards\nwhat it has seen.\n\nIn support of this, it appears that the variance achievable by the\ndefault random forest comfortably fits the small samples, but is not\nas explanatory when more samples challenge it. This is seen in how the\nmaximum error and the large B-alloy set's rmse worsen with larger\npartition sizes.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Obtain Generality Baseline\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The four main alloy classes are similarly represented in the dataset,\nSo, it is interesting to know if a model trained on only three of them\nperforms well on the fourth.\n\nTo aid in this, the unrefined model is tested on a group-wise\ncross-validation k-fold split to get a baseline performance. The final\nrefined model will be tested in the same way, but using the dedicated\ntest split. Of all the architectures developed in these notebooks, the\narchitecture that performs best in this final evaluation will go on to\nserve as the genetic algorithm's surrogate\n\nthe folds used by this group-aware partition splitter are different\nfrom the number of folds decided on using the learning curves. Here,\neach fold consists entirely of members of ONE group. There are four\nmain groups, so we use four partition. The tiny \"BandX\" group is mixed\nin with the others.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["gkf = GroupKFold(n_splits=4)"]},{"cell_type":"markdown","metadata":{},"source":["A function is defined to streamline this test and ensure it cannot\ncontaminate the estimator to be optimized\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def test_generality(estimator, X_tr, y_tr, groups_tr, X_ts, y_ts, groups_ts):\n    estimator = clone(estimator) #unfitted, cloned params\n    scores = []\n    for train_idx, val_idx in gkf.split(X_tr, y_tr, groups=groups_tr):\n        tr_val_group_names = groups_tr.iloc[val_idx].index.get_level_values(\"mix\").unique()\n        estimator.fit(X_tr.iloc[train_idx], y_tr.iloc[train_idx])\n        tr_val_score_series = pd.Series(batch_score(estimator, X_tr.iloc[val_idx], y_tr.iloc[val_idx], **scorings))\n        tr_val_score_series.name=\"_&_\".join(tr_val_group_names)\n        scores.append(tr_val_score_series)\n    tr_val_scores = pd.concat(scores, axis=1).assign(partition=\"validation\")\n    scores = []\n    for _, val_idx in gkf.split(X_ts, y_ts, groups=groups_ts):\n        ts_group_names = groups_ts.iloc[val_idx].index.get_level_values(\"mix\").unique()\n        ts_score_series = pd.Series(batch_score(estimator, X_ts.iloc[val_idx], y_ts.iloc[val_idx], **scorings))\n        ts_score_series.name=\"_&_\".join(ts_group_names)\n        scores.append(ts_score_series)\n    ts_scores = pd.concat(scores, axis=1).assign(partition=\"test\")\n    group_scores = pd.concat([tr_val_scores, ts_scores]).round(5).drop_duplicates(keep=\"first\")\n    return group_scores"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"]},{"cell_type":"markdown","metadata":{},"source":["These are validation scores.\n\n-   Unsurprisingly, there is substantial error in all the groups in this case.\n-   B discrepancy interpretations:\n    -   Again, the variance in the B site alloy type is difficult to capture with\n        the current parameters.\n    -   Also, possibly, The B partition is mostly representative of the others\n\nTo elaborate, when the B partition is excluded from training, the model\nfails to generalize, but when it is included it generalizes pretty well.\n\nNote: batch<sub>score</sub> defaults to unweighted scoring if the sum of weights\nin the given sample equal zero, that is why most of the group-wise rmse\nscores are the same.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Optimize Hyper-parameters for Composition Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### define first level of Hyperparameter search\n\n"]},{"cell_type":"markdown","metadata":{},"source":["These cells archive the optimization process, running them will only\nserve to illustrate the process taken to pick the parameters that\nmoved on to the following iterations.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct original Hyper-parameter Space\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#\"max_depth\": [10, 20, 40],\n#\"min_samples_split\": [2, 5, 10]\ngrid = [\n    {'normalizer__norm': ['l1', 'l2', 'max'],\n     'randomforestregressor__bootstrap': [True], #build each tree from sample\n     'randomforestregressor__ccp_alpha': [0.0, 0.001, 0.002], #cost-complexity pruning\n     'randomforestregressor__criterion': ['mse', 'mae'], #['squared_error', 'poisson'], #update sklearn and try these\n     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n     'randomforestregressor__max_samples': [0.9, 0.6, 0.3], #frac to bag\n     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #0.3 corresponds to the onset of aggressive ccp\n     'randomforestregressor__min_impurity_split': [None],\n     'randomforestregressor__min_samples_leaf': [1], #just sensible\n     'randomforestregressor__min_samples_split': [2, 5], #\n     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n     'randomforestregressor__n_estimators': [20, 50, 100],\n     'randomforestregressor__n_jobs': [4], #parallelize exec\n     'randomforestregressor__oob_score': [True], #use out-of-bag samples to validate (faster)\n     'randomforestregressor__random_state': [None], #do not touch\n     'randomforestregressor__verbose': [0], \n     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n     },\n    {'normalizer__norm': ['l1', 'l2', 'max'],\n     'randomforestregressor__bootstrap': [False], #Build each tree from everything\n     'randomforestregressor__ccp_alpha': [0.0, 0.001, 0.002], #cost-complexity pruning\n     'randomforestregressor__criterion': ['mse', 'mae'], #['squared_error', 'poisson'],\n     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n     'randomforestregressor__max_samples': [None], #\"bag\" everything\n     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #\n     'randomforestregressor__min_impurity_split': [None],\n     'randomforestregressor__min_samples_leaf': [1], #just sensible\n     'randomforestregressor__min_samples_split': [2, 5], #\n     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n     'randomforestregressor__n_estimators': [20, 50, 100],\n     'randomforestregressor__n_jobs': [4], #parallelize exec\n     #oob score not available\n     'randomforestregressor__random_state': [None], #do not touch\n     'randomforestregressor__verbose': [0], \n     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n     }\n]"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Composition model of PBE<sub>BG</sub>\n\n"]},{"cell_type":"markdown","metadata":{},"source":["initially, only 3 fold validation is used to save on computation time\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cgs = gsCV(estimator=cpipe,\n            param_grid=grid,\n            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\ncgs.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["Fitting 3 folds for each of 10368 candidates, totalling 31104 fits\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Determine next Grid Space to explore\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\nsummary"]},{"cell_type":"markdown","metadata":{},"source":["In summary:\n\n| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n|---|---|---|---|---|---|---|---|---|\n| normalizer_<sub>norm</sub>|[l1, l2, max]|[l1, l2, max]|1.082051|1.082051|[16.36, 9.28, 4.44]|[16.36, 9.28, 4.44]|[l1]|[l1]|\n| bootstrap|[True]|[False]|0.264410|0.365092|[16.57]|[13.51]|[True]|[False]|\n| ccp<sub>alpha</sub>|[0.0, 0.001, 0.002]|[0.0, 0.001, 0.002]|0.847720|0.847720|[17.76, 9.72, 2.59]|[17.76, 9.72, 2.59]|[0.0]|[0.0]|\n| criterion|[mse, mae]|[mse, mae]|0.677494|0.677494|[12.26, 17.82]|[12.26, 17.82]|[mae]|[mae]|\n| max<sub>depth</sub>|[25, 20]|[25, 20]|0.686211|0.686211|[11.56, 18.52]|[11.56, 18.52]|[20]|[20]|\n| max<sub>features</sub>|[auto, 3, 5]|[auto, 3, 5]|0.969392|0.969392|[10.38, 5.12, 14.58]|[10.38, 5.12, 14.58]|[auto, 5]|[auto, 5]|\n| max<sub>leaf</sub><sub>nodes</sub>|[750, 800]|[750, 800]|0.682295|0.682295|[19.36, 10.72]|[19.36, 10.72]|[750]|[750]|\n| max<sub>samples</sub>|[0.9, 0.6, 0.3]|[None]|0.820702|0.365092|[12.81, 1.59, 2.17]|[13.51]|[0.9]|[None]|\n| min<sub>impurity</sub><sub>decrease</sub>|[0.0, 0.3]|[0.0, 0.3]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| min<sub>impurity</sub><sub>split</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n| min<sub>samples</sub><sub>split</sub>|[2, 5]|[2, 5]|0.657692|0.657692|[22.72, 7.36]|[22.72, 7.36]|[2]|[2]|\n| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.085233|1.085233|[9.72, 10.54, 9.82]|[9.72, 10.54, 9.82]|[50]|[50]|\n| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n| oob<sub>score</sub>|[True]|NaN|0.264410|NaN|[16.57]|NaN|[True]|NaN|\n| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n\nWith a first eshaustive search and mostly uniform score weights:\n\n-   l1 normalization is much more performant in preparing the domain for random forest\n-   mae (more expensive) significantly higher performing &#x2013; suggesting the tree works better when node splits are not biased by extremes.\n-   bootstrapping the regressor is marginally more performant\n    -   90% sampling is best so far (rfr improves with more exposure, makes sense)\n    -   notice: bootstrap sampling appears to rank more frequently in the\n        top ten, but no-bootstrap has similar aggregate scores, suggesting\n        it appears fewer times but consistently outranks bootstrapping\n-   ccp and impurity decrease thresholds do not appear to positively impact performance, however other tree growth limiters do\n    -   a node limit of 750 significantly outperformed a relatively unlimited 800\n    -   a depth limit of 20 subdevisions significantly outperformed relatively unlimited 25\n        (these limit numbers were obtained from depth analysis on a simple DT)\n    -   unlimited splitting is significantly more performant that preventing splits below 5 samples/leaf\n        (less ability to fit outliers in a split is not helping to improve these scores)\n-   So far, more DT estimators is better than less\n\nwhen B<sub>rmse</sub> score is weighted more heavily:\n\n-   settings that favor model variance are even better\n    -   22.72 > 7.36 -> 2 > 5 min samples/leaf compared to 19.9>7.06\n-   boot strap sampling increases its lead on no-bootstrap by 1 point.\n-   20 estimators actually ranks much higher! less averaging => more bias\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### &#x2013; Iteratively Optimize Hyperparameters\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The dominant setting are kept, but another exhaustive search with\nother bounds on max depth and max leaf nodes.\n\nmax feature granularity is increased in the successful range\n\nn estimators is a sensitive parameters. it should be checked for\npotential overfitting in the performant range\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['mae'],\n     'randomforestregressor__max_depth': [15, 20],\n     'randomforestregressor__max_features': ['auto', 5, 10],\n     'randomforestregressor__max_leaf_nodes': [700, 750],\n     'randomforestregressor__max_samples': [0.9], #continuing to give oob validation it's best chance will probably result in bootstraping appearing more often in the top,\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_impurity_split': [None],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [20, 50, 100],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]},\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [False], #nevertheless, non-bootstrapped model previously scored high despite their decreased frequency in the top 10.\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['mae'],\n     'randomforestregressor__max_depth': [15, 20],\n     'randomforestregressor__max_features': ['auto', 5, 10],\n     'randomforestregressor__max_leaf_nodes': [700, 750],\n     'randomforestregressor__max_samples': [None],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_impurity_split': [None],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [20, 50, 100],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]}\n]"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Composition model of PBE<sub>BG</sub>\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cgs = gsCV(estimator=cpipe,\n            param_grid=grid,\n            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\ncgs.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["Fitting 3 folds for each of 72 candidates, totalling 216 fits\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Determine next Grid Space to explore\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\nsummary"]},{"cell_type":"markdown","metadata":{},"source":["In summary:\n\n| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n|---|---|---|---|---|---|---|---|---|\n| normalizer_<sub>norm</sub>|[l1]|[l1]|-0.000000|-0.000000|NaN|NaN|[l1]|[l1]|\n| bootstrap|[True]|[False]|0.210918|0.343871|[29.23]|[7.06]|[True]|[False]|\n| ccp<sub>alpha</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| criterion|[mae]|[mae]|-0.000000|-0.000000|NaN|NaN|[mae]|[mae]|\n| max<sub>depth</sub>|[15, 20]|[15, 20]|0.675143|0.675143|[16.32, 19.98]|[16.32, 19.98]|[20]|[20]|\n| max<sub>features</sub>|[auto, 5, 10]|[auto, 5, 10]|1.097888|1.097888|[13.86, 11.49, 10.95]|[13.86, 11.49, 10.95]|[auto]|[auto]|\n| max<sub>leaf</sub><sub>nodes</sub>|[700, 750]|[700, 750]|0.692782|0.692782|[19.07, 17.22]|[19.07, 17.22]|[700]|[700]|\n| max<sub>samples</sub>|[0.9]|[None]|0.210918|0.343871|[29.23]|[7.06]|[0.9]|[None]|\n| min<sub>impurity</sub><sub>decrease</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| min<sub>impurity</sub><sub>split</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n| min<sub>samples</sub><sub>split</sub>|[2]|[2]|-0.000000|-0.000000|NaN|NaN|[2]|[2]|\n| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.076753|1.076753|[7.67, 10.28, 18.34]|[7.67, 10.28, 18.34]|[100]|[100]|\n| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n| oob<sub>score</sub>|[True]|NaN|0.210918|NaN|[29.23]|NaN|[True]|NaN|\n| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n\nIn an upset, the non-bootstrapped approach losses standing.\n\n-   non-bootstrapped regressions appear less frequently and score much worse when focusing on strongly performing settings\n    -   bootstrapping means the model artifically gains more perspective\n        by randomly subsamling the training set within each fold per\n        estimator, and it uses the excluded \"out of bag\" samples to\n        contribute to the validation score.\n-   optimal tree depth is undecided between 15, 20, and 25 (last round) but 20 has won both rounds narrowly\n-   unlike before, trees using all features are most performant. 5 was better than 2 by a lot, but 10 and 'auto'=14 better still.\n-   the inclination to use more estimators is better ustified now\n\nwhen weighting B scores, all of this remain true.\n\n-   more estimators => better even more so\n-   bootstrapping is even better\n-   limits on tree depth are still ambiguous\n-   however, max features is less clear cut. B scores probably favor models that look at mostly the 6 B composition vectors.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### &#x2013; Iteratively Optimize Hyperparameters\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"markdown","metadata":{},"source":["bootsrapping is decidedly better at this point.\n\nThere does not appear to be any good reason to restrict the\ncomposition dimensions available to the model\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['mae'],\n     'randomforestregressor__max_depth': [15, 20, 25],\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [650, 700, 750],\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_impurity_split': [None],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [50, 75, 100, 125],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]},\n]"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Composition model of PBE<sub>BG</sub>\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cgs = gsCV(estimator=cpipe,\n            param_grid=grid,\n            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\ncgs.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["totalling 108 fits.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Determine next Grid Space to explore\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,1,1,0,1], strategy=\"oavg\")\nsummary"]},{"cell_type":"markdown","metadata":{},"source":["In summary:\n\n| |space<sub>0</sub>|entropy<sub>0</sub>|scores<sub>0</sub>|next<sub>0</sub>|\n|---|---|---|---|---|\n| normalizer_<sub>norm</sub>|[l1]|-0.000000|NaN|[l1]|\n| bootstrap|[True]|-0.000000|NaN|[True]|\n| ccp<sub>alpha</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| criterion|[mae]|-0.000000|NaN|[mae]|\n| max<sub>depth</sub>|[15, 20, 25]|1.094351|[12.92, 7.42, 12.23]|[15, 25]|\n| max<sub>features</sub>|[auto]|-0.000000|NaN|[auto]|\n| max<sub>leaf</sub><sub>nodes</sub>|[650, 700, 750]|1.097582|[14.43, 9.28, 8.86]|[650]|\n| max<sub>samples</sub>|[0.9]|-0.000000|NaN|[0.9]|\n| min<sub>impurity</sub><sub>decrease</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| min<sub>impurity</sub><sub>split</sub>|[None]|-0.000000|NaN|[None]|\n| min<sub>samples</sub><sub>leaf</sub>|[1]|-0.000000|NaN|[1]|\n| min<sub>samples</sub><sub>split</sub>|[2]|-0.000000|NaN|[2]|\n| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| n<sub>estimators</sub>|[50, 75, 100, 125]|1.380655|[10.9, 7.72, 7.78, 6.17]|[50]|\n| n<sub>jobs</sub>|[4]|-0.000000|NaN|[4]|\n| oob<sub>score</sub>|[True]|-0.000000|NaN|[True]|\n| random<sub>state</sub>|[None]|-0.000000|NaN|[None]|\n| verbose|[0]|-0.000000|NaN|[0]|\n| warm<sub>start</sub>|[False]|-0.000000|NaN|[False]|\n\n-   substantially limiting nodes appears to be good\n-   either strong or lax limits on depth are favored.\n-   suddenly less estimators are prefered.\n    -   n<sub>estimators</sub> is a dominant setting. It is also more conceptually clear.\n\nSo, n<sub>estimators</sub> should be individually optimized at this point, in\norder to decide the tree growth limits\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Perform a sensitivity analysis in this optimal subspace\n\n"]},{"cell_type":"markdown","metadata":{},"source":["plot a validation curve for n-estimators when considering unlimited trees parameters.\n\nIf limiting trees on top of an \"optimal\" ensemble size improves the model, good.\n\nAdjusting the ensemble size to fit the tree optimizations is not as\nmuch a sensible use of the RFR architecture's strengths.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['mae'],\n     'randomforestregressor__max_depth': [None], #unlimited\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [None], #unlimited\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_impurity_split': [None],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [100],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]},\n]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#set the parameters settled in the grid. n estimators will be handled by the validation curve function in the next step\ncpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})"]},{"cell_type":"markdown","metadata":{},"source":["##### n<sub>estimators</sub> validation scan\n\n"]},{"cell_type":"markdown","metadata":{},"source":["using the 4 fold cross validation established by the LC analysis\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["VC = pvc(validation_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n         param_name='randomforestregressor__n_estimators', param_range=np.linspace(50, 150, 15).astype(int), cv=4, scoring=scorings)\nVC = VC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p = sns.FacetGrid(VC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\np.map(sns.lineplot, 'randomforestregressor__n_estimators', \"value\")\np.add_legend()"]},{"cell_type":"markdown","metadata":{},"source":["unfortunately, it seems adjustments to ensemble size have no\nappreciable effect at these settings. this is a good indication that\nthis data does not suit the RFR architecture.\n\nThis visualization also helps to clarify that the current model space\nhas reasonable RMSE, but nevertheless consistently permits at least one\nlarge outlier.\n\nThere's not much point in continuing. with more minor optimizations if\nthis parameter is ineffective. There is also huge variability within\neach fold. The band here is a 95% confidence interval. Trying to pick\na estimator that narrows this band is not helpful at this point\nbecause sometimes the performance is actually good with the wide band,\nwhereas the mean performance is catastrophically bad everywhere\n\nwe're likely to see better with other models. let's wrap up and move on\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Best Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Parametrize\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['mae'],\n     'randomforestregressor__max_depth': [None], #unlimited\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [None], #unlimited\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_impurity_split': [None],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [100], #better performance by somome metrics, about as good as it gets\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]},\n]"]},{"cell_type":"markdown","metadata":{},"source":["#### Train Final Estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})\ncpipe.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["#### evaluate\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p, data = parityplot(cpipe, mc_ts, my_ts.PBE_bg_eV.to_frame(), aspect=1.0, hue=\"mix\")\np.figure.show()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p.savefig(\"./ParityPlots/rfr_c_bg.png\", transparent=True)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["pd.Series(batch_score(cpipe, mc_tr, my_tr.PBE_bg_eV, **scorings)).to_frame()"]},{"cell_type":"markdown","metadata":{},"source":["Luckily, the RFR designed here does fit the test data well. Even\ndespite the possible shortcomings in the architecture.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Obtain Generality Measure\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"]},{"cell_type":"markdown","metadata":{},"source":["The post-optimization RFR generalizes across all alloy types many\ntimes better than the default parameters.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exporting/Importing Trained Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### save the model for distribution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Never load joblib/pickle files that you do not trust, they can execute\narbitrary code on your computer.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["joblib.dump(cpipe, \"./Models/rfr_c_opt.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["### load if needed\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cpipe = joblib.load(\"./Models/rfr_c_opt.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["## Compute Site-Averaged Properties Vectors\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mp = mc.ft.derive_from(lookup, \"element\", \"Formula\")"]},{"cell_type":"markdown","metadata":{},"source":["## Model BG Using Site-Averaged Properties\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Make Properties Pipeline\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ppipe = mkpipe(StandardScaler(), RandomForestRegressor())"]},{"cell_type":"markdown","metadata":{},"source":["## comp+prop model opt\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
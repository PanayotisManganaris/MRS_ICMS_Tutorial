{"cells":[{"cell_type":"markdown","metadata":{},"source":"Optimizing RFR Models based on Perovskite Compositions\n======================================================\n\n**Author:** Panayotis Manganaris\n\n"},{"cell_type":"markdown","metadata":{},"source":["## dependencies\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# featurization\nimport cmcl\nfrom cmcl import Categories\n# multi-criterion model evaluation\nfrom yogi.model_selection import summarize_HPO\nfrom yogi.model_selection import pandas_validation_curve as pvc\nfrom yogi.metrics.pandas_scoring import PandasScoreAdaptor as PSA\nfrom yogi.metrics.pandas_scoring import batch_score\n# visualization convenience\nfrom spyglass.model_imaging import parityplot"]},{"cell_type":"markdown","metadata":{},"source":["The Intel distribution provides accelerated ml algorithms. Run this\ncell before importing the algorithms\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearnex import patch_sklearn\npatch_sklearn()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# data tools\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n# feature engineering\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, Normalizer, StandardScaler\n# predictors\nfrom sklearn.ensemble import RandomForestRegressor\n## pipeline workflow\nfrom sklearn.pipeline import make_pipeline as mkpipe\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV as gsCV\n# model eval\nfrom sklearn.base import clone\nfrom sklearn.metrics import make_scorer, mean_squared_error, r2_score, explained_variance_score, max_error\nimport joblib\n#visualization\nfrom sklearn import set_config\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# ignore all FutureWarnings -- handling coming in a future version of yogi\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data and Compute Composition Vectors\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Load Curated Subset of Mannodi Data\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["my = pd.read_csv(\"./mannodi_data.csv\").set_index([\"index\", \"Formula\", \"sim_cell\"])\nlookup = pd.read_csv(\"./constituent_properties.csv\").set_index(\"Formula\")"]},{"cell_type":"markdown","metadata":{},"source":["### Compute Compostion Vectors using cmcl\n\n"]},{"cell_type":"markdown","metadata":{},"source":["additionally, cmcl offers a convenience function for creating groups\nout of the column labels\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mc = my.ft.comp() # compute numerical compostion vectors from strings\nmc = mc.collect.abx() # convenient site groupings for perovskites data"]},{"cell_type":"markdown","metadata":{},"source":["### generate mix category and assign to index for future imaging\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Helps with visualizing the final model performance by tracking the\nfeatures through all the model development transformations\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mixlog = mc.groupby(level=0, axis=1).count()\nmix = mixlog.pipe(Categories.logif, condition=lambda x: x>1, default=\"pure\", catstring=\"and\")\nmc = mc.assign(mix=mix).set_index(\"mix\", append=True)\nmy = my.assign(mix=mix).set_index(\"mix\", append=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Model BG Using Composition Vectors\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Make Composition Pipeline\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The model we use relies on a standard chain of data preprocessing\nsteps. These steps are encapsulated in a single meta-estimator called\na \"pipeline.\" The pipeline is instantiated with default \"hyperparameters.\"\n\nLater, a hyperparameter optimization strategy based on exhaustive grid\nsearch will chose parameters that ensure the architecture performs as\nbest it can for the purposes of inverse design.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fillna = SimpleImputer(strategy=\"constant\", fill_value=0.0)\ncpipe = mkpipe(fillna, Normalizer(), RandomForestRegressor())"]},{"cell_type":"markdown","metadata":{},"source":["### Scoring Scheme\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The composition space is approximated by a set of discrete domains due\nthe limited nature of the 2x2x2 supercells used to obtain this\ndata. The pure domain is completely covered by 90 data points. The\nalloy domains are combinatorial large in the 14 dimensional component\nspace under focus, and each has only been sparsely sampled.\n\nOur primary objective is to create a surrogate model of these domains\nthat can be used in active learning.\n\nIn order to target model generality, it will be necessary to score the\nmodel's performance with respect to it's predictions individually over\neach individual alloy domain and over the union of these domains.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### prepare subset scoring weights and ordinal group labels\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mixweight = pd.get_dummies(mix)\nmixcat = pd.Series(OrdinalEncoder().fit_transform(mix.values.reshape(-1, 1)).reshape(-1),\n                     index=mc.index).astype(int)"]},{"cell_type":"markdown","metadata":{},"source":["#### Define Scoring Metrics\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Nine metrics are used to monitor the fitness of the random forest model.\nsome monitor the data trend and spread\n\n-   R<sup>2</sup>\n-   Explained Variance\n\none monitors the largest breakdown in accuracy.\n\n-   Max Error\n\nsix group-wise RMSE metrics monitor monitor accuracy for each \n\n-   total RMSE\n-   A-site RMSE\n-   B-site RMSE\n-   X-site RMSE\n-   XandB-site RMSE\n-   pure RMSE\n\nthe PandasScoreAdaptor (PSA) ensures the prediction losses are\nweighted correctly when scoring so long as both the targets and the\nsample weights passed to the estimators are always pandas objects\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["site_mse = PSA(mean_squared_error).score\nscorings = {'r2': make_scorer(r2_score),\n            'ev': make_scorer(explained_variance_score),\n            'maxerr': make_scorer(max_error, greater_is_better=False),\n            'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n            'A_rmse': make_scorer(site_mse, greater_is_better=False,\n                                  squared=False, sample_weight=mixweight.A),\n            'B_rmse': make_scorer(site_mse, greater_is_better=False,\n                                  squared=False, sample_weight=mixweight.B),\n            'X_rmse': make_scorer(site_mse, greater_is_better=False,\n                                  squared=False, sample_weight=mixweight.X),\n            'BandX_rmse': make_scorer(site_mse, greater_is_better=False,\n                                      squared=False, sample_weight=mixweight.BandX),\n            'Pure_rmse': make_scorer(site_mse, greater_is_better=False,\n                                     squared=False, sample_weight=mixweight.pure),}"]},{"cell_type":"markdown","metadata":{},"source":["### Make Dedicated Test Train Split\n\n"]},{"cell_type":"markdown","metadata":{},"source":["A dedicated test-train split is made. This split preserves the\nproportion of each alloy group in the test and train partitions, which\nhelps with the final model evaluation. \n\n-   all decisions about model optimization will be made using only the dedicated training partition\n-   The test partition will be reserved until a final model pipeline is parametrized and fit\n-   the predictions made on the test partition will either confirm or deny the model's ability to work outside of the training domain\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)\ntrain_idx, test_idx = next(sss.split(mc, mixcat)) #stratify split by mix categories\nmc_tr, mc_ts = mc.iloc[train_idx], mc.iloc[test_idx]\nmy_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]\nmixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]"]},{"cell_type":"markdown","metadata":{},"source":["### Learning Curves &#x2013; Using Deterministically Random Cross Validation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Cross-validation within the training set will be the only way of\nchecking the generality of models aside from testing. So, first, it is\nnecessary to have some understanding of how much data is needed for\ntraining to have a chance of generalizing.\n\n10-fold splits makes for 10 sample scores at each partition\nsize. Also, up to 90% of the training set is dedicated to training. If\nthe model fails with this level of exposure, it will be hard use.\n\nThe shuffling is seeded with a deterministic random state to ensure\nscores are comparable across partition size\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["with joblib.parallel_backend('multiprocessing'):\n  LC = pvc(learning_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n           train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc, scoring=scorings)\n  LC = LC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["Notice that the error metrics are negated so that, consistently with\nthe R<sup>2</sup> and ev scores, the greater the number, the better the model\nperforms.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p = sns.FacetGrid(LC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\np.map(sns.lineplot, \"train_sizes\", \"value\")\np.add_legend()\np.figure.show()"]},{"cell_type":"markdown","metadata":{},"source":["It appears that 3-4 fold cross-validation is sufficient (training with\n300/400 or 260/400 points, validating with the compliment)\n\nThe Random Forest's validation scores continue to rise as the partition size grows\n\n-   Random Forest generality increases with more exposure\n-   Equivalently, an insufficiently experienced random forest is biased towards what it has seen.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Obtain Generality Baseline\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The four main alloy classes are similarly represented in the dataset,\nSo, it is interesting to know if a model trained on only three of them\nperforms well on the fourth.\n\nThere are four main groups, so we use four groupwise partitions. The\ntiny \"BandX\" group is mixed in with the others. The splitter uses the\nmixcat ordinal series to ensure the index splits are correct.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["gkf = GroupKFold(n_splits=4)"]},{"cell_type":"markdown","metadata":{},"source":["A function is defined to streamline this test and ensure it cannot\ncontaminate the estimator to be optimized\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def test_generality(estimator, X_tr, y_tr, groups_tr, X_ts, y_ts, groups_ts):\n    estimator = clone(estimator) #unfitted, cloned params\n    gentpl = gkf.split(X_tr, y_tr, groups=groups_tr), gkf.split(X_ts, y_ts, groups=groups_ts)\n    #train and test index generators, in order\n    val_scores = []\n    tst_scores = []\n    for train_idx, val_idx, _, tst_idx in [sum(gengroup, ()) for gengroup in zip(*gentpl)]:\n        tr_val_group_names = groups_tr.iloc[val_idx].index.get_level_values(\"mix\").unique()\n        ts_group_names = groups_ts.iloc[tst_idx].index.get_level_values(\"mix\").unique()\n        #fit to tr part\n        estimator.fit(X_tr.iloc[train_idx], y_tr.iloc[train_idx])\n        #get val and test scores\n        tr_val_score_series = pd.Series(batch_score(estimator, X_tr.iloc[val_idx], y_tr.iloc[val_idx], **scorings))\n        tr_val_score_series.name=\"_&_\".join(tr_val_group_names)\n        ts_score_series = pd.Series(batch_score(estimator, X_ts.iloc[tst_idx], y_ts.iloc[tst_idx], **scorings))\n        ts_score_series.name=\"_&_\".join(ts_group_names)\n        val_scores.append(tr_val_score_series)\n        tst_scores.append(ts_score_series)\n    tr_val_scores = pd.concat(val_scores, axis=1).assign(partition=\"validation\")\n    ts_scores = pd.concat(tst_scores, axis=1).assign(partition=\"test\")\n    group_scores = pd.concat([tr_val_scores, ts_scores]).round(5).drop_duplicates(keep=\"first\")\n    return group_scores"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"]},{"cell_type":"markdown","metadata":{},"source":["-   There are substantial errors in all the groups\n    -   in the test partition, the model utterly fails in extrapolating the bandgaps of B-mixed alloys\n    -   in the train partition, it's still not good\n    -   however, extrapolating the other partitions is relatively much better\n-   The B partition is mostly representative of the others\n    -   w.r.t the underlying function as identified by this specific model architecture\n    -   different architectures can be inclined to learn different characteristics of a function\n\n**Note:** batch<sub>score</sub> defaults to unweighted scoring if the sum of weights\nin the given sample equal zero, that is why most of the group-wise rmse\nscores are the same.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Optimize Hyper-parameters for Composition Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### define first level of Hyperparameter search\n\n"]},{"cell_type":"markdown","metadata":{},"source":["These cells archive the optimization process, running them is not\nnecessary. The result is saved in the notebook at the end\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct original Hyper-parameter Space\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1', 'l2', 'max'],\n     'randomforestregressor__bootstrap': [True], #build each tree from sample\n     'randomforestregressor__ccp_alpha': [0.0, 0.002], #cost-complexity pruning\n     'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'poisson'], #variance reductions vs deviance reduction\n     #'randomforestregressor__maxBins:': [256],\n     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n     'randomforestregressor__max_samples': [0.9, 0.6, 0.3], #frac to bag\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #0.3 corresponds to the onset of aggressive ccp\n     'randomforestregressor__min_samples_leaf': [1], #just sensible\n     'randomforestregressor__min_samples_split': [2, 5], #\n     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n     'randomforestregressor__n_estimators': [20, 50, 100],\n     'randomforestregressor__n_jobs': [4], #parallelize exec\n     'randomforestregressor__oob_score': [True], #use out-of-bag samples to validate (faster)\n     'randomforestregressor__random_state': [None], #do not touch\n     'randomforestregressor__verbose': [0], \n     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n     },\n    {'normalizer__norm': ['l1', 'l2', 'max'],\n     'randomforestregressor__bootstrap': [False], #Build each tree from everything\n     'randomforestregressor__ccp_alpha': [0.0, 0.002], #cost-complexity pruning\n     'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'poisson'], #variance reductions vs deviance reduction\n     #'randomforestregressor__maxBins:': [256],\n     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n     'randomforestregressor__max_samples': [None], #\"bag\" everything\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #\n     'randomforestregressor__min_samples_leaf': [1], #just sensible\n     'randomforestregressor__min_samples_split': [2, 5], #\n     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n     'randomforestregressor__n_estimators': [20, 50, 100],\n     'randomforestregressor__n_jobs': [4], #parallelize exec\n     #oob score not available\n     'randomforestregressor__random_state': [None], #do not touch\n     'randomforestregressor__verbose': [0], \n     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n     }\n]"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Composition model of PBE<sub>BG</sub>\n\n"]},{"cell_type":"markdown","metadata":{},"source":["initially, only 3 fold validation is used to save on computation time\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cgs = gsCV(estimator=cpipe,\n            param_grid=grid,\n            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n\nwith joblib.parallel_backend('multiprocessing'):\n    cgs.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["Fitting 3 folds for each of 10368 candidates, totalling 31104 fits &#x2013; about 45 minutes to fit with acceleration.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Determine next Grid Space to explore\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\nsummary"]},{"cell_type":"markdown","metadata":{},"source":["Mostly Equal Weights In Summary:\n\n| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n|---|---|---|---|---|---|---|---|---|\n| normalizer_<sub>norm</sub>|[l1, l2, max]|[l1, l2, max]|0.853155|0.853155|[19.07, 0.07, 10.74]|[19.07, 0.07, 10.74]|[l1, max]|[l1, max]|\n| bootstrap|[True]|[False]|0.142457|0.289120|[26.18]|[3.69]|[True]|[False]|\n| ccp<sub>alpha</sub>|[0.0, 0.002]|[0.0, 0.002]|0.485723|0.485723|[27.33, 2.54]|[27.33, 2.54]|[0.0]|[0.0]|\n| criterion|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|0.687787|0.687787|[18.12, 11.75, 0.0]|[18.12, 11.75, 0.0]|[squared<sub>error</sub>, absolute<sub>error</sub>]|[squared<sub>error</sub>, absolute<sub>error</sub>]|\n| max<sub>depth</sub>|[25, 20]|[25, 20]|0.683604|0.683604|[14.16, 15.72]|[14.16, 15.72]|[20]|[20]|\n| max<sub>features</sub>|[auto, 3, 5]|[auto, 3, 5]|0.759206|0.759206|[25.04, 0.06, 4.78]|[25.04, 0.06, 4.78]|[auto]|[auto]|\n| max<sub>leaf</sub><sub>nodes</sub>|[750, 800]|[750, 800]|0.692553|0.692553|[14.88, 15.0]|[14.88, 15.0]|[800]|[800]|\n| max<sub>samples</sub>|[0.9, 0.6, 0.3]|[None]|0.812818|0.289120|[21.28, 4.82, 0.09]|[0.16]|[0.9]|[None]|\n| minBinSize|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n| min<sub>impurity</sub><sub>decrease</sub>|[0.0, 0.3]|[0.0, 0.3]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n| min<sub>samples</sub><sub>split</sub>|[2, 5]|[2, 5]|0.485723|0.485723|[26.1, 3.77]|[26.1, 3.77]|[2]|[2]|\n| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.083522|1.083522|[7.95, 8.22, 13.7]|[7.95, 8.22, 13.7]|[100]|[100]|\n| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n| oob<sub>score</sub>|[True]|NaN|0.142457|NaN|[0.84]|NaN|[True]|NaN|\n| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n\n-   l1 normalization is best\n-   bootstrapping the regressor is much more performant\n    -   90% sampling is best (rfr improves with more exposure, makes sense)\n    -   notice: bootstrap sampling appears to rank only slightly more\n        frequently in the top ten than no-bootstrap, but has much higher\n        scores. suggesting it also dominates the highest ranks in general.\n-   max normalization also does well, but not as well\n-   squared error does best\n-   absolute<sub>error</sub> (more expensive) is less susceptible to compromising on extremes, but appears mostly unfavorable\n-   limiting tree depth slightly better than not limiting it\n-   growth on all features better than growth on few features. larger axis limits yet to be explored\n-   unlimited nodes marginally better than limited nodes\n-   impurity decrease threshold is ineffective\n-   unlimited split granularity better than limited granularity\n-   generally, more estimators outperform fewer\n\nFavorably Weighting B scores:\n\n| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n|---|---|---|---|---|---|---|---|---|\n| normalizer_<sub>norm</sub>|[l1, l2, max]|[l1, l2, max]|0.853155|0.853155|[21.94, 0.07, 11.25]|[21.94, 0.07, 11.25]|[l1, max]|[l1, max]|\n| bootstrap|[True]|[False]|0.142457|0.289120|[29.47]|[3.79]|[True]|[False]|\n| ccp<sub>alpha</sub>|[0.0, 0.002]|[0.0, 0.002]|0.485723|0.485723|[30.68, 2.58]|[30.68, 2.58]|[0.0]|[0.0]|\n| criterion|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|0.687787|0.687787|[21.39, 11.87, 0.0]|[21.39, 11.87, 0.0]|[squared<sub>error</sub>, absolute<sub>error</sub>]|[squared<sub>error</sub>, absolute<sub>error</sub>]|\n| max<sub>depth</sub>|[25, 20]|[25, 20]|0.683604|0.683604|[16.54, 16.72]|[16.54, 16.72]|[20]|[20]|\n| max<sub>features</sub>|[auto, 3, 5]|[auto, 3, 5]|0.759206|0.759206|[27.43, 0.06, 5.77]|[27.43, 0.06, 5.77]|[auto]|[auto]|\n| max<sub>leaf</sub><sub>nodes</sub>|[750, 800]|[750, 800]|0.692553|0.692553|[17.34, 15.92]|[17.34, 15.92]|[750]|[750]|\n| max<sub>samples</sub>|[0.9, 0.6, 0.3]|[None]|0.812818|0.289120|[22.68, 6.69, 0.09]|[0.16]|[0.9]|[None]|\n| minBinSize|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n| min<sub>impurity</sub><sub>decrease</sub>|[0.0, 0.3]|[0.0, 0.3]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n| min<sub>samples</sub><sub>split</sub>|[2, 5]|[2, 5]|0.485723|0.485723|[28.39, 4.87]|[28.39, 4.87]|[2]|[2]|\n| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.083522|1.083522|[9.59, 8.5, 15.18]|[9.59, 8.5, 15.18]|[100]|[100]|\n| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n| oob<sub>score</sub>|[True]|NaN|0.142457|NaN|[0.84]|NaN|[True]|NaN|\n| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n\n-   gap between limited and unlimited tree depth closes slightly\n-   limited leaf nodes becomes more favorable than unlimited leaf nodes &#x2013; reversal!\n-   20 estimators actually ranks much higher. less averaging => more bias helps B\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### &#x2013; Iteratively Optimize Hyperparameters\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"markdown","metadata":{},"source":["next<sub>grid</sub> helps set up the next exhaustive search\n\n-   l1 normalization is chosen\n-   bootstrapping is chosen\n-   squared and absolute error compete again\n-   depth limits are broadened &#x2013; explore strong limits and no limits\n-   larger feature access limits are tried\n-   strong limits on leaf notes are tried along with no limits and moderate limits\n-   recommendations taken for others\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['squared_error', 'absolute_error'],\n     'randomforestregressor__max_depth': [15, 20, None], #broadening search\n     'randomforestregressor__max_features': ['auto', 10], #larger limit\n     'randomforestregressor__max_leaf_nodes': [700, 800, None], #broadening search\n     'randomforestregressor__max_samples': [0.9], #gives bootstrapping it's best chance\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [50, 100, 150], #broadening search\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]},\n]"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Composition model of PBE<sub>BG</sub>\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cgs = gsCV(estimator=cpipe,\n            param_grid=grid,\n            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n\nwith joblib.parallel_backend('multiprocessing'):\n    cgs.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["Fitting 3 folds for each of 108 candidates, totalling 324 fits &#x2013; just a few minutes\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Determine next Grid Space to explore\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\nsummary"]},{"cell_type":"markdown","metadata":{},"source":["Mostly Equal Weights In Summary:\n\n| |space<sub>0</sub>|entropy<sub>0</sub>|scores<sub>0</sub>|next<sub>0</sub>|\n|---|---|---|---|---|\n| normalizer_<sub>norm</sub>|[l1]|-0.000000|NaN|[l1]|\n| bootstrap|[True]|-0.000000|NaN|[True]|\n| ccp<sub>alpha</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| criterion|[squared<sub>error</sub>, absolute<sub>error</sub>]|0.673012|[26.46, 9.32]|[squared<sub>error</sub>]|\n| max<sub>depth</sub>|[15, 20, None]|1.092669|[11.65, 10.56, 0.33]|[15, 20]|\n| max<sub>features</sub>|[auto, 10]|0.650818|[21.97, 13.81]|[auto]|\n| max<sub>leaf</sub><sub>nodes</sub>|[700, 800, None]|1.092669|[9.65, 8.09, 0.38]|[700, 800]|\n| max<sub>samples</sub>|[0.9]|-0.000000|NaN|[0.9]|\n| minBinSize|[1]|-0.000000|NaN|[1]|\n| min<sub>impurity</sub><sub>decrease</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| min<sub>samples</sub><sub>leaf</sub>|[1]|-0.000000|NaN|[1]|\n| min<sub>samples</sub><sub>split</sub>|[2]|-0.000000|NaN|[2]|\n| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| n<sub>estimators</sub>|[50, 100, 150]|1.094257|[8.31, 13.14, 14.32]|[100, 150]|\n| n<sub>jobs</sub>|[4]|-0.000000|NaN|[4]|\n| oob<sub>score</sub>|[True]|-0.000000|NaN|[True]|\n| random<sub>state</sub>|[None]|-0.000000|NaN|[None]|\n| verbose|[0]|-0.000000|NaN|[0]|\n| warm<sub>start</sub>|[False]|-0.000000|NaN|[False]|\n\n-   squared error still better than absolute error, but absolute error is still fairly common\n-   strong limits on depth perform better than unlimited depth &#x2013; limits tree bias\n-   unlimited feature access still better than fewer\n-   limited nodes actually outperforms unlimited. more aggressive limits are better\n-   150 estimators outperforms 100 &#x2013; sensitivity is likely\n\nmore heavily weighting B scores make no change to interpretation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### &#x2013; Iteratively Optimize Hyperparameters\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"markdown","metadata":{},"source":["in the limited tree domains, without interference from n<sub>estimators</sub>, various criterion are explored in detail\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'poisson'],\n     'randomforestregressor__max_depth': [15, 20],\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [600, 700, 800],\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [100], #compromise in anticipation of possible overfitting\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]\n     }\n]"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Composition model of PBE<sub>BG</sub>\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cgs = gsCV(estimator=cpipe,\n            param_grid=grid,\n            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n\nwith joblib.parallel_backend('multiprocessing'):\n    cgs.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["Fitting 3 folds for each of 18 candidates, totalling 54 fits\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  Determine next Grid Space to explore\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\nsummary"]},{"cell_type":"markdown","metadata":{},"source":["In Summary:\n\n| |space<sub>0</sub>|entropy<sub>0</sub>|scores<sub>0</sub>|next<sub>0</sub>|\n|---|---|---|---|---|\n| normalizer_<sub>norm</sub>|[l1]|-0.000000|NaN|[l1]|\n| bootstrap|[True]|-0.000000|NaN|[True]|\n| ccp<sub>alpha</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| criterion|[squared<sub>error</sub>, absolute<sub>error</sub>, poisson]|1.054920|[18.27, 14.65, 2.16]|[squared<sub>error</sub>, absolute<sub>error</sub>]|\n| max<sub>depth</sub>|[15, 20]|0.673012|[10.35, 24.73]|[20]|\n| max<sub>features</sub>|[auto]|-0.000000|NaN|[auto]|\n| max<sub>leaf</sub><sub>nodes</sub>|[600, 700, 800]|1.098612|[8.45, 12.28, 14.35]|[700, 800]|\n| max<sub>samples</sub>|[0.9]|-0.000000|NaN|[0.9]|\n| minBinSize|[1]|-0.000000|NaN|[1]|\n| min<sub>impurity</sub><sub>decrease</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| min<sub>samples</sub><sub>leaf</sub>|[1]|-0.000000|NaN|[1]|\n| min<sub>samples</sub><sub>split</sub>|[2]|-0.000000|NaN|[2]|\n| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|-0.000000|NaN|[0.0]|\n| n<sub>estimators</sub>|[100]|-0.000000|NaN|[100]|\n| n<sub>jobs</sub>|[4]|-0.000000|NaN|[4]|\n| oob<sub>score</sub>|[True]|-0.000000|NaN|[True]|\n| random<sub>state</sub>|[None]|-0.000000|NaN|[None]|\n| verbose|[0]|-0.000000|NaN|[0]|\n| warm<sub>start</sub>|[False]|-0.000000|NaN|[False]|\n\n-   squared error and absolute error perform similarly in this area\n    -   absolute error is more expensive to use,\n    -   choosing square error is justifiable for practicality\n-   gentle limits are preferred to hard limits\n\nB score weighting makes no difference.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Perform sensitivity analysis for n<sub>estimators</sub> in optimal subspace\n\n"]},{"cell_type":"markdown","metadata":{},"source":["plot a validation curve over n<sub>estimators</sub> when considering reasonable\ntree parameters.\n\nThe model is probably more sensitive to n<sub>estimators</sub>.\n\nAdjusting the ensemble size to fit the tree optimizations is not as\nmuch a sensible use of the RFR architecture's strengths, so it is used\nfirst.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['squared_error'],\n     'randomforestregressor__max_depth': [20],\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [700],\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [100],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]\n     }\n]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})"]},{"cell_type":"markdown","metadata":{},"source":["##### n<sub>estimators</sub> validation scan\n\n"]},{"cell_type":"markdown","metadata":{},"source":["using the 4 fold cross validation established by the LC analysis\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["with joblib.parallel_backend('multiprocessing'):\n  VC = pvc(validation_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n           param_name='randomforestregressor__n_estimators', param_range=np.linspace(50, 150, 15).astype(int), cv=4, scoring=scorings)\n  VC = VC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p = sns.FacetGrid(VC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\np.map(sns.lineplot, 'randomforestregressor__n_estimators', \"value\")\np.add_legend()\np.figure.show()"]},{"cell_type":"markdown","metadata":{},"source":["-   Model scores in this parameter subspace appear to be insensitive to n<sub>estimators</sub>. This is a plesant surprise.\n-   maxerror is consistently better between 110 and 120 estimators &#x2013; this is chosen going forward\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Perform sensitivity analysis for max<sub>leaf</sub><sub>nodes</sub> in optimal subspace\n\n"]},{"cell_type":"markdown","metadata":{},"source":["plot a validation curve over max<sub>leaf</sub><sub>nodes</sub> when considering optimal\nforest parameters.\n\nIf limiting trees on top of an optimal ensemble size improves the\nmodel, good. If not, no loss.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['squared_error'],\n     'randomforestregressor__max_depth': [20],\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [700],\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [115],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]\n     }\n]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})"]},{"cell_type":"markdown","metadata":{},"source":["##### max<sub>leaf</sub><sub>nodes</sub> validation scan\n\n"]},{"cell_type":"markdown","metadata":{},"source":["using the 4 fold cross validation established by the LC analysis\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["with joblib.parallel_backend('multiprocessing'):\n  VC = pvc(validation_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n           param_name='randomforestregressor__max_leaf_nodes', param_range=np.linspace(600, 800, 15).astype(int), cv=4, scoring=scorings)\n  VC = VC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p = sns.FacetGrid(VC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\np.map(sns.lineplot, 'randomforestregressor__max_leaf_nodes', \"value\")\np.add_legend()\np.figure.show()"]},{"cell_type":"markdown","metadata":{},"source":["-   Model scores in this parameter subspace appear to be mostly insensitive to max<sub>leaf</sub><sub>nodes</sub>.\n    -   training scores seem to improve with less restriction\n    -   validation scores are mostly flat\n    -   validation score variability does tend to increase\n    -   to attempt to tighten performance, 650-670 appears best\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Perform sensitivity analysis for max<sub>depth</sub> in optimal subspace\n\n"]},{"cell_type":"markdown","metadata":{},"source":["another possibly sensitive tree parameter, likely of low consequence\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### 1.  construct subsequent HP space\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['squared_error'],\n     'randomforestregressor__max_depth': [20],\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [660],\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [115],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]\n     }\n]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})"]},{"cell_type":"markdown","metadata":{},"source":["##### max<sub>depth</sub> validation scan\n\n"]},{"cell_type":"markdown","metadata":{},"source":["using the 4 fold cross validation established by the LC analysis\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["with joblib.parallel_backend('multiprocessing'):\n  VC = pvc(validation_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n           param_name='randomforestregressor__max_depth', param_range=np.linspace(15, 25, 15).astype(int), cv=4, scoring=scorings)\n  VC = VC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["p = sns.FacetGrid(VC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\np.map(sns.lineplot, 'randomforestregressor__max_depth', \"value\")\np.add_legend()\np.figure.show()"]},{"cell_type":"markdown","metadata":{},"source":["-   Model scores in this parameter subspace are again mostly insensitive to max<sub>depth</sub>.\n    -   training scores are fine\n    -   validation scores are mostly flat\n    -   the two are also reasonably close to eachother\n    -   validation score variability does tend to increase\n    -   to attempt to tighten performance, 20 or 22 appears best\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Best Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Parametrize\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["grid = [\n    {'normalizer__norm': ['l1'],\n     'randomforestregressor__bootstrap': [True],\n     'randomforestregressor__ccp_alpha': [0.0],\n     'randomforestregressor__criterion': ['squared_error'],\n     'randomforestregressor__max_depth': [22],\n     'randomforestregressor__max_features': ['auto'],\n     'randomforestregressor__max_leaf_nodes': [660],\n     'randomforestregressor__max_samples': [0.9],\n     'randomforestregressor__minBinSize': [1],\n     'randomforestregressor__min_impurity_decrease': [0.0],\n     'randomforestregressor__min_samples_leaf': [1],\n     'randomforestregressor__min_samples_split': [2],\n     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n     'randomforestregressor__n_estimators': [115],\n     'randomforestregressor__n_jobs': [4],\n     'randomforestregressor__oob_score': [True],\n     'randomforestregressor__random_state': [None],\n     'randomforestregressor__verbose': [0],\n     'randomforestregressor__warm_start': [False]\n     }\n]"]},{"cell_type":"markdown","metadata":{},"source":["#### Train Final Estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})\ncpipe.fit(mc_tr, my_tr.PBE_bg_eV)"]},{"cell_type":"markdown","metadata":{},"source":["#### evaluate\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#change between tr and ts suffixes to see test vs train pairity plot\np, data = parityplot(cpipe, mc_ts, my_ts.PBE_bg_eV.to_frame(), aspect=1.0, hue=\"mix\")\np.figure.show()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#optionally save the figure for presentation\np.savefig(\"./ParityPlots/rfr_bg_c_opt.png\", transparent=True)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#change between tr and ts suffixes to see test vs train scores -- both are good\npd.Series(batch_score(cpipe, mc_ts, my_ts.PBE_bg_eV, **scorings)).to_frame()"]},{"cell_type":"markdown","metadata":{},"source":["The RFR designed here scores consistently well with the test data,\nHowever, parity visual could indicate nonetheless show possible\nover fitting to the training data if the spread is not consistent.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Obtain Generality Measure\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"]},{"cell_type":"markdown","metadata":{},"source":["The post-optimization RFR generalizes across all alloy types much\nbetter than the default parameters, but there are clearly limitations.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exporting/Importing Trained Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### save the model for distribution\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Never load joblib/pickle files that you do not trust, they can execute\narbitrary code on your computer.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["joblib.dump(cpipe, \"./Models/rfr_c_opt.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["### load if needed\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["cpipe = joblib.load(\"./Models/rfr_c_opt.joblib\")"]},{"cell_type":"markdown","metadata":{},"source":["## Compute Site-Averaged Properties Vectors\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["mp = mc.ft.derive_from(lookup, \"element\", \"Formula\")"]},{"cell_type":"markdown","metadata":{},"source":["## Model BG Using Site-Averaged Properties\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Make Properties Pipeline\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["ppipe = mkpipe(StandardScaler(), RandomForestRegressor())"]},{"cell_type":"markdown","metadata":{},"source":["## comp+prop model opt\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
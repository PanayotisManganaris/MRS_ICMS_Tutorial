{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing RFR Models based on Perovskite Compositions\n",
    "======================================================\n",
    "\n",
    "**Author:** Panayotis Manganaris\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurization\n",
    "import cmcl\n",
    "from cmcl import Categories\n",
    "# multi-criterion model evaluation\n",
    "from yogi.model_selection import summarize_HPO\n",
    "from yogi.model_selection import pandas_validation_curve as pvc\n",
    "from yogi.metrics.pandas_scoring import PandasScoreAdaptor as PSA\n",
    "from yogi.metrics.pandas_scoring import batch_score\n",
    "# visualization convenience\n",
    "from spyglass.model_imaging import parityplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intel distribution provides accelerated ml algorithms. Run this\n",
    "cell before importing the algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tools\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# feature engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, Normalizer, StandardScaler\n",
    "# predictors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "## pipeline workflow\n",
    "from sklearn.pipeline import make_pipeline as mkpipe\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV as gsCV\n",
    "from sklearn.base import clone\n",
    "# model eval\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, explained_variance_score, max_error\n",
    "#visualization\n",
    "from sklearn import set_config\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Compute Composition Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Curated Subset of Mannodi Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = pd.read_csv(\"./mannodi_data.csv\").set_index([\"index\", \"Formula\", \"sim_cell\"])\n",
    "lookup = pd.read_csv(\"./constituent_properties.csv\").set_index(\"Formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Compostion Vectors using cmcl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additionally, cmcl offers a convenience function for creating groups\n",
    "out of the column labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = my.ft.comp() # compute numerical compostion vectors from strings\n",
    "mc = mc.collect.abx() # convenient site groupings for perovskites data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate mix category and assign to index for future imaging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this helps with visualizing the final model performance by tracking\n",
    "the features through all the model development transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixlog = mc.groupby(level=0, axis=1).count()\n",
    "mix = mixlog.pipe(Categories.logif, condition=lambda x: x>1, default=\"pure\", catstring=\"and\")\n",
    "mc = mc.assign(mix=mix).set_index(\"mix\", append=True)\n",
    "my = my.assign(mix=mix).set_index(\"mix\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BG Using Composition Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Composition Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we use relies on a standard chain of data preprocessing\n",
    "steps. These steps will be encapsulated in a single meta-estimator\n",
    "called a \"pipeline.\"\n",
    "\n",
    "This pipeline is constructed to handle the expected form of the 14\n",
    "dimensional composition space our data covers. The regression is only\n",
    "performed after an imputer treats N/A constituent quantities as zero\n",
    "and a normalizer transformer ensures a bounded domain. The regressor\n",
    "itself is initialized now using only default \"hyperparameters.\"\n",
    "\n",
    "Hyperparameters control aspects of the regression algorithm's behavior\n",
    "that are not learned from the data. Later, a hyperparameter\n",
    "optimization strategy based on exhaustive grid search will chose\n",
    "parameters that ensure the architecture performs as best it can for\n",
    "the purposes of inverse design.\n",
    "\n",
    "The process for determining how suitable a model is for this purpose\n",
    "is discussed next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "cpipe = mkpipe(fillna, Normalizer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Scheme\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset posits a challenge to successful modeling. We seek to\n",
    "create a regressor which generalizes well to perovskites of any alloy\n",
    "character. As seen in the visualizations, the dataset contains mainly\n",
    "pure, A, B and X-site alloyed perovskites. Some of which are defined\n",
    "mostly on axes with substantially differing statistical profiles.\n",
    "Thereby inviting a model to fit to artifacts of the sample\n",
    "distributions rather than physically meaningful information. Starting\n",
    "now, great care will be taken to mitigate this.\n",
    "\n",
    "The composition space is approximated by a set of discrete domains due\n",
    "the limited nature of the 2x2x2 supercells used to obtain this\n",
    "data. The pure domain is completely covered by 90 data points. The\n",
    "alloy domains are combinatorial large in the 14 dimensional component\n",
    "space under focus, and each has only been sparsely sampled.\n",
    "\n",
    "Recall, our primary objective is to create a surrogate model of these\n",
    "domains that can inform future computations needed to exhaustively\n",
    "explore it.\n",
    "\n",
    "In order to validate that our model generalizes well, it will be\n",
    "necessary to score the model's performance with respect to it's\n",
    "predictions individually over each alloy domain simultaneously with\n",
    "it's prediction over the union of these domains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare subset scoring weights and ordinal group labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixweight = pd.get_dummies(mix)\n",
    "mixcat = pd.Series(OrdinalEncoder().fit_transform(mix.values.reshape(-1, 1)).reshape(-1),\n",
    "                     index=mc.index).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Scoring Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine metrics are used to monitor the fitness of the random forest model.\n",
    "\n",
    "-   R<sup>2</sup> and Explained Variance scores keep track of the regression's ability to capture the data trend and spread\n",
    "-   Max Error helps to keep track the largest breakdown in accuracy.\n",
    "-   Additionally six group-wise RMSE metrics are kept.\n",
    "    -   total RMSE\n",
    "    -   A-site RMSE\n",
    "    -   B-site RMSE\n",
    "    -   X-site RMSE\n",
    "    -   XandB-site RMSE\n",
    "    -   pure RMSE\n",
    "\n",
    "the PandasScoreAdaptor (PSA) ensures the prediction losses are\n",
    "weighted correctly when scoring so long as both the targets and the\n",
    "sample weights passed to the estimators are always pandas objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_mse = PSA(mean_squared_error).score\n",
    "scorings = {'r2': make_scorer(r2_score),\n",
    "            'ev': make_scorer(explained_variance_score),\n",
    "            'maxerr': make_scorer(max_error, greater_is_better=False),\n",
    "            'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n",
    "            'A_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.A),\n",
    "            'B_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.B),\n",
    "            'X_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.X),\n",
    "            'BandX_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                      squared=False, sample_weight=mixweight.BandX),\n",
    "            'Pure_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                     squared=False, sample_weight=mixweight.pure),}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dedicated Test Train Split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, a dedicated test-train split is made. This split\n",
    "preserves the proportion of each alloy group in the test and train\n",
    "partitions, which helps with the final model evaluation.\n",
    "\n",
    "Beyond this point, all decisions about model optimization will be made\n",
    "using only the dedicated training partition. The test partition will\n",
    "be reserved until a final model pipeline is parametrized and\n",
    "fit. Then, the predictions made on the test partition will either\n",
    "confirm or deny the model's ability to work outside of the training\n",
    "domain. The model that ultimately scores best in this respect will be\n",
    "used to perform inverse design.\n",
    "\n",
    "The partition is essentially arbitrary. In this case, just enough test\n",
    "data is held aside to represent at least one sample of each alloy\n",
    "type in the final tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)\n",
    "train_idx, test_idx = next(sss.split(mc, mixcat)) #stratify split by mix categories\n",
    "mc_tr, mc_ts = mc.iloc[train_idx], mc.iloc[test_idx]\n",
    "my_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]\n",
    "mixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves &#x2013; Using Deterministically Random Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation within the training set will be the only way of\n",
    "checking the generality of any given model instance. So, first, it is\n",
    "necessary to have some understanding of how much data is needed to\n",
    "train the model in order for it to have a chance of generalizing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a splitter is set to produce a 10 cross validation folds on\n",
    "randomly shuffled data indices. This means that 9 examples of a fit\n",
    "model will be will be produced for each partition size to be tested.\n",
    "The 10th fold will likewise be checked 9 times for each corresponding\n",
    "training. So, effectively at each of the partition sizes tested, the\n",
    "model is given 9 chances to perform after seeing 90% of the data. This\n",
    "is generous. If the model fails to generalize under these conditions\n",
    "at some partition size, it is probably a lost cause.\n",
    "\n",
    "Notice, the splitter will also perform internal shuffling of the data\n",
    "indices. (The shuffling occurs before the splitting in each\n",
    "case). This helps to prevent the model from training on the ordered\n",
    "groupings in the dataset. For these learning curves, the splitter is\n",
    "initialized with a deterministic random state. This ensures that each\n",
    "fold generated at each partition size in the learning curve is not\n",
    "only comparable to the others in its split, but to those in\n",
    "neighboring partition sizes as well.\n",
    "\n",
    "In the future, when performing hyperparameter optimization, the cv\n",
    "splitters used will use the global random state. This will help to\n",
    "ensure that the shuffle seen by the myriad fit calls will not be\n",
    "treated itself as a hyperparameter to be optimized. The eventual final\n",
    "model should therefore be robust to the order of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LC = pvc(learning_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n",
    "         train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc, scoring=scorings)\n",
    "LC = LC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the error metrics are negated so that, consistently with\n",
    "the R<sup>2</sup> and ev scores, the greater the number, the better the model\n",
    "performs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.FacetGrid(LC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\n",
    "p.map(sns.lineplot, \"train_sizes\", \"value\")\n",
    "p.add_legend()\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that 3-4 fold cross-validation is sufficient (training with\n",
    "300/400 or 260/400 points, validating with the compliment)\n",
    "\n",
    "The learning curve indicates a few things about the bias and variance\n",
    "of the model in question. The Random Forest's validation scores\n",
    "continue to rise as the partition size grows. This implies that the\n",
    "model's generality likely increases with more exposure. Equivalently,\n",
    "an insufficiently experienced random forest tends to be biased towards\n",
    "what it has seen.\n",
    "\n",
    "In support of this, it appears that the variance achievable by the\n",
    "default random forest comfortably fits the small samples, but is not\n",
    "as explanatory when more samples challenge it. This is seen in how the\n",
    "maximum error and the large B-alloy set's rmse worsen with larger\n",
    "partition sizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Generality Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four main alloy classes are similarly represented in the dataset,\n",
    "So, it is interesting to know if a model trained on only three of them\n",
    "performs well on the fourth.\n",
    "\n",
    "To aid in this, the unrefined model is tested on a group-wise\n",
    "cross-validation k-fold split to get a baseline performance. The final\n",
    "refined model will be tested in the same way, but using the dedicated\n",
    "test split. Of all the architectures developed in these notebooks, the\n",
    "architecture that performs best in this final evaluation will go on to\n",
    "serve as the genetic algorithm's surrogate\n",
    "\n",
    "the folds used by this group-aware partition splitter are different\n",
    "from the number of folds decided on using the learning curves. Here,\n",
    "each fold consists entirely of members of ONE group. There are four\n",
    "main groups, so we use four partition. The tiny \"BandX\" group is mixed\n",
    "in with the others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is defined to streamline this test and ensure it cannot\n",
    "contaminate the estimator to be optimized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generality(estimator, X_tr, y_tr, groups_tr, X_ts, y_ts, groups_ts):\n",
    "    estimator = clone(estimator) #unfitted, cloned params\n",
    "    scores = []\n",
    "    for train_idx, val_idx in gkf.split(X_tr, y_tr, groups=groups_tr):\n",
    "        tr_val_group_names = groups_tr.iloc[val_idx].index.get_level_values(\"mix\").unique()\n",
    "        estimator.fit(X_tr.iloc[train_idx], y_tr.iloc[train_idx])\n",
    "        tr_val_score_series = pd.Series(batch_score(estimator, X_tr.iloc[val_idx], y_tr.iloc[val_idx], **scorings))\n",
    "        tr_val_score_series.name=\"_&_\".join(tr_val_group_names)\n",
    "        scores.append(tr_val_score_series)\n",
    "    tr_val_scores = pd.concat(scores, axis=1).assign(partition=\"validation\")\n",
    "    scores = []\n",
    "    for _, val_idx in gkf.split(X_ts, y_ts, groups=groups_ts):\n",
    "        ts_group_names = groups_ts.iloc[val_idx].index.get_level_values(\"mix\").unique()\n",
    "        ts_score_series = pd.Series(batch_score(estimator, X_ts.iloc[val_idx], y_ts.iloc[val_idx], **scorings))\n",
    "        ts_score_series.name=\"_&_\".join(ts_group_names)\n",
    "        scores.append(ts_score_series)\n",
    "    ts_scores = pd.concat(scores, axis=1).assign(partition=\"test\")\n",
    "    group_scores = pd.concat([tr_val_scores, ts_scores]).round(5).drop_duplicates(keep=\"first\")\n",
    "    return group_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are validation scores.\n",
    "\n",
    "-   Unsurprisingly, there is substantial error in all the groups in this case.\n",
    "-   B discrepancy interpretations:\n",
    "    -   Again, the variance in the B site alloy type is difficult to capture with\n",
    "        the current parameters.\n",
    "    -   Also, possibly, The B partition is mostly representative of the others\n",
    "\n",
    "To elaborate, when the B partition is excluded from training, the model\n",
    "fails to generalize, but when it is included it generalizes pretty well.\n",
    "\n",
    "Note: batch<sub>score</sub> defaults to unweighted scoring if the sum of weights\n",
    "in the given sample equal zero, that is why most of the group-wise rmse\n",
    "scores are the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Hyper-parameters for Composition Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define first level of Hyperparameter search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells archive the optimization process, running them will only\n",
    "serve to illustrate the process taken to pick the parameters that\n",
    "moved on to the following iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  construct original Hyper-parameter Space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"max_depth\": [10, 20, 40],\n",
    "#\"min_samples_split\": [2, 5, 10]\n",
    "grid = [\n",
    "    {'normalizer__norm': ['l1', 'l2', 'max'],\n",
    "     'randomforestregressor__bootstrap': [True], #build each tree from sample\n",
    "     'randomforestregressor__ccp_alpha': [0.0, 0.001, 0.002], #cost-complexity pruning\n",
    "     'randomforestregressor__criterion': ['mse', 'mae'], #['squared_error', 'poisson'], #update sklearn and try these\n",
    "     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n",
    "     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n",
    "     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n",
    "     'randomforestregressor__max_samples': [0.9, 0.6, 0.3], #frac to bag\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #0.3 corresponds to the onset of aggressive ccp\n",
    "     'randomforestregressor__min_impurity_split': [None],\n",
    "     'randomforestregressor__min_samples_leaf': [1], #just sensible\n",
    "     'randomforestregressor__min_samples_split': [2, 5], #\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n",
    "     'randomforestregressor__n_estimators': [20, 50, 100],\n",
    "     'randomforestregressor__n_jobs': [4], #parallelize exec\n",
    "     'randomforestregressor__oob_score': [True], #use out-of-bag samples to validate (faster)\n",
    "     'randomforestregressor__random_state': [None], #do not touch\n",
    "     'randomforestregressor__verbose': [0], \n",
    "     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n",
    "     },\n",
    "    {'normalizer__norm': ['l1', 'l2', 'max'],\n",
    "     'randomforestregressor__bootstrap': [False], #Build each tree from everything\n",
    "     'randomforestregressor__ccp_alpha': [0.0, 0.001, 0.002], #cost-complexity pruning\n",
    "     'randomforestregressor__criterion': ['mse', 'mae'], #['squared_error', 'poisson'],\n",
    "     'randomforestregressor__max_depth': [25, 20], #investigate dept of constituent trees, limit\n",
    "     'randomforestregressor__max_features': ['auto', 3, 5], #split after considering\n",
    "     'randomforestregressor__max_leaf_nodes': [750, 800], #see depth exploration in DT notes\n",
    "     'randomforestregressor__max_samples': [None], #\"bag\" everything\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0, 0.3], #\n",
    "     'randomforestregressor__min_impurity_split': [None],\n",
    "     'randomforestregressor__min_samples_leaf': [1], #just sensible\n",
    "     'randomforestregressor__min_samples_split': [2, 5], #\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0], #\n",
    "     'randomforestregressor__n_estimators': [20, 50, 100],\n",
    "     'randomforestregressor__n_jobs': [4], #parallelize exec\n",
    "     #oob score not available\n",
    "     'randomforestregressor__random_state': [None], #do not touch\n",
    "     'randomforestregressor__verbose': [0], \n",
    "     'randomforestregressor__warm_start': [False] #make a new forest every time (honest)\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  Composition model of PBE<sub>BG</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initially, only 3 fold validation is used to save on computation time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgs = gsCV(estimator=cpipe,\n",
    "            param_grid=grid,\n",
    "            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n",
    "cgs.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 10368 candidates, totalling 31104 fits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  Determine next Grid Space to explore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| normalizer_<sub>norm</sub>|[l1, l2, max]|[l1, l2, max]|1.082051|1.082051|[16.36, 9.28, 4.44]|[16.36, 9.28, 4.44]|[l1]|[l1]|\n",
    "| bootstrap|[True]|[False]|0.264410|0.365092|[16.57]|[13.51]|[True]|[False]|\n",
    "| ccp<sub>alpha</sub>|[0.0, 0.001, 0.002]|[0.0, 0.001, 0.002]|0.847720|0.847720|[17.76, 9.72, 2.59]|[17.76, 9.72, 2.59]|[0.0]|[0.0]|\n",
    "| criterion|[mse, mae]|[mse, mae]|0.677494|0.677494|[12.26, 17.82]|[12.26, 17.82]|[mae]|[mae]|\n",
    "| max<sub>depth</sub>|[25, 20]|[25, 20]|0.686211|0.686211|[11.56, 18.52]|[11.56, 18.52]|[20]|[20]|\n",
    "| max<sub>features</sub>|[auto, 3, 5]|[auto, 3, 5]|0.969392|0.969392|[10.38, 5.12, 14.58]|[10.38, 5.12, 14.58]|[auto, 5]|[auto, 5]|\n",
    "| max<sub>leaf</sub><sub>nodes</sub>|[750, 800]|[750, 800]|0.682295|0.682295|[19.36, 10.72]|[19.36, 10.72]|[750]|[750]|\n",
    "| max<sub>samples</sub>|[0.9, 0.6, 0.3]|[None]|0.820702|0.365092|[12.81, 1.59, 2.17]|[13.51]|[0.9]|[None]|\n",
    "| min<sub>impurity</sub><sub>decrease</sub>|[0.0, 0.3]|[0.0, 0.3]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| min<sub>impurity</sub><sub>split</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n",
    "| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n",
    "| min<sub>samples</sub><sub>split</sub>|[2, 5]|[2, 5]|0.657692|0.657692|[22.72, 7.36]|[22.72, 7.36]|[2]|[2]|\n",
    "| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.085233|1.085233|[9.72, 10.54, 9.82]|[9.72, 10.54, 9.82]|[50]|[50]|\n",
    "| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n",
    "| oob<sub>score</sub>|[True]|NaN|0.264410|NaN|[16.57]|NaN|[True]|NaN|\n",
    "| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n",
    "| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n",
    "| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n",
    "\n",
    "With a first eshaustive search and mostly uniform score weights:\n",
    "\n",
    "-   l1 normalization is much more performant in preparing the domain for random forest\n",
    "-   mae (more expensive) significantly higher performing &#x2013; suggesting the tree works better when node splits are not biased by extremes.\n",
    "-   bootstrapping the regressor is marginally more performant\n",
    "    -   90% sampling is best so far (rfr improves with more exposure, makes sense)\n",
    "    -   notice: bootstrap sampling appears to rank more frequently in the\n",
    "        top ten, but no-bootstrap has similar aggregate scores, suggesting\n",
    "        it appears fewer times but consistently outranks bootstrapping\n",
    "-   ccp and impurity decrease thresholds do not appear to positively impact performance, however other tree growth limiters do\n",
    "    -   a node limit of 750 significantly outperformed a relatively unlimited 800\n",
    "    -   a depth limit of 20 subdevisions significantly outperformed relatively unlimited 25\n",
    "        (these limit numbers were obtained from depth analysis on a simple DT)\n",
    "    -   unlimited splitting is significantly more performant that preventing splits below 5 samples/leaf\n",
    "        (less ability to fit outliers in a split is not helping to improve these scores)\n",
    "-   So far, more DT estimators is better than less\n",
    "\n",
    "when B<sub>rmse</sub> score is weighted more heavily:\n",
    "\n",
    "-   settings that favor model variance are even better\n",
    "    -   22.72 > 7.36 -> 2 > 5 min samples/leaf compared to 19.9>7.06\n",
    "-   boot strap sampling increases its lead on no-bootstrap by 1 point.\n",
    "-   20 estimators actually ranks much higher! less averaging => more bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x2013; Iteratively Optimize Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  construct subsequent HP space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominant setting are kept, but another exhaustive search with\n",
    "other bounds on max depth and max leaf nodes.\n",
    "\n",
    "max feature granularity is increased in the successful range\n",
    "\n",
    "n estimators is a sensitive parameters. it should be checked for\n",
    "potential overfitting in the performant range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [True],\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['mae'],\n",
    "     'randomforestregressor__max_depth': [15, 20],\n",
    "     'randomforestregressor__max_features': ['auto', 5, 10],\n",
    "     'randomforestregressor__max_leaf_nodes': [700, 750],\n",
    "     'randomforestregressor__max_samples': [0.9], #continuing to give oob validation it's best chance will probably result in bootstraping appearing more often in the top,\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_impurity_split': [None],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [20, 50, 100],\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__oob_score': [True],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]},\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [False], #nevertheless, non-bootstrapped model previously scored high despite their decreased frequency in the top 10.\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['mae'],\n",
    "     'randomforestregressor__max_depth': [15, 20],\n",
    "     'randomforestregressor__max_features': ['auto', 5, 10],\n",
    "     'randomforestregressor__max_leaf_nodes': [700, 750],\n",
    "     'randomforestregressor__max_samples': [None],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_impurity_split': [None],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [20, 50, 100],\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  Composition model of PBE<sub>BG</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgs = gsCV(estimator=cpipe,\n",
    "            param_grid=grid,\n",
    "            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n",
    "cgs.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  Determine next Grid Space to explore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,2,1,0,1], strategy=\"oavg\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "| |space<sub>0</sub>|space<sub>1</sub>|entropy<sub>0</sub>|entropy<sub>1</sub>|scores<sub>0</sub>|scores<sub>1</sub>|next<sub>0</sub>|next<sub>1</sub>|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| normalizer_<sub>norm</sub>|[l1]|[l1]|-0.000000|-0.000000|NaN|NaN|[l1]|[l1]|\n",
    "| bootstrap|[True]|[False]|0.210918|0.343871|[29.23]|[7.06]|[True]|[False]|\n",
    "| ccp<sub>alpha</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| criterion|[mae]|[mae]|-0.000000|-0.000000|NaN|NaN|[mae]|[mae]|\n",
    "| max<sub>depth</sub>|[15, 20]|[15, 20]|0.675143|0.675143|[16.32, 19.98]|[16.32, 19.98]|[20]|[20]|\n",
    "| max<sub>features</sub>|[auto, 5, 10]|[auto, 5, 10]|1.097888|1.097888|[13.86, 11.49, 10.95]|[13.86, 11.49, 10.95]|[auto]|[auto]|\n",
    "| max<sub>leaf</sub><sub>nodes</sub>|[700, 750]|[700, 750]|0.692782|0.692782|[19.07, 17.22]|[19.07, 17.22]|[700]|[700]|\n",
    "| max<sub>samples</sub>|[0.9]|[None]|0.210918|0.343871|[29.23]|[7.06]|[0.9]|[None]|\n",
    "| min<sub>impurity</sub><sub>decrease</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| min<sub>impurity</sub><sub>split</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n",
    "| min<sub>samples</sub><sub>leaf</sub>|[1]|[1]|-0.000000|-0.000000|NaN|NaN|[1]|[1]|\n",
    "| min<sub>samples</sub><sub>split</sub>|[2]|[2]|-0.000000|-0.000000|NaN|NaN|[2]|[2]|\n",
    "| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|[0.0]|-0.000000|-0.000000|NaN|NaN|[0.0]|[0.0]|\n",
    "| n<sub>estimators</sub>|[20, 50, 100]|[20, 50, 100]|1.076753|1.076753|[7.67, 10.28, 18.34]|[7.67, 10.28, 18.34]|[100]|[100]|\n",
    "| n<sub>jobs</sub>|[4]|[4]|-0.000000|-0.000000|NaN|NaN|[4]|[4]|\n",
    "| oob<sub>score</sub>|[True]|NaN|0.210918|NaN|[29.23]|NaN|[True]|NaN|\n",
    "| random<sub>state</sub>|[None]|[None]|-0.000000|-0.000000|NaN|NaN|[None]|[None]|\n",
    "| verbose|[0]|[0]|-0.000000|-0.000000|NaN|NaN|[0]|[0]|\n",
    "| warm<sub>start</sub>|[False]|[False]|-0.000000|-0.000000|NaN|NaN|[False]|[False]|\n",
    "\n",
    "In an upset, the non-bootstrapped approach losses standing.\n",
    "\n",
    "-   non-bootstrapped regressions appear less frequently and score much worse when focusing on strongly performing settings\n",
    "    -   bootstrapping means the model artifically gains more perspective\n",
    "        by randomly subsamling the training set within each fold per\n",
    "        estimator, and it uses the excluded \"out of bag\" samples to\n",
    "        contribute to the validation score.\n",
    "-   optimal tree depth is undecided between 15, 20, and 25 (last round) but 20 has won both rounds narrowly\n",
    "-   unlike before, trees using all features are most performant. 5 was better than 2 by a lot, but 10 and 'auto'=14 better still.\n",
    "-   the inclination to use more estimators is better ustified now\n",
    "\n",
    "when weighting B scores, all of this remain true.\n",
    "\n",
    "-   more estimators => better even more so\n",
    "-   bootstrapping is even better\n",
    "-   limits on tree depth are still ambiguous\n",
    "-   however, max features is less clear cut. B scores probably favor models that look at mostly the 6 B composition vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x2013; Iteratively Optimize Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  construct subsequent HP space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootsrapping is decidedly better at this point.\n",
    "\n",
    "There does not appear to be any good reason to restrict the\n",
    "composition dimensions available to the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [True],\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['mae'],\n",
    "     'randomforestregressor__max_depth': [15, 20, 25],\n",
    "     'randomforestregressor__max_features': ['auto'],\n",
    "     'randomforestregressor__max_leaf_nodes': [650, 700, 750],\n",
    "     'randomforestregressor__max_samples': [0.9],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_impurity_split': [None],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [50, 75, 100, 125],\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__oob_score': [True],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  Composition model of PBE<sub>BG</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgs = gsCV(estimator=cpipe,\n",
    "            param_grid=grid,\n",
    "            cv=3, verbose=1, scoring=scorings, refit=\"r2\", return_train_score=True)\n",
    "cgs.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "totalling 108 fits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  Determine next Grid Space to explore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, next_grid = summarize_HPO(cgs, grid, topN=10, metric_weights=[1,1,1,1,1,1,1,0,1], strategy=\"oavg\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "| |space<sub>0</sub>|entropy<sub>0</sub>|scores<sub>0</sub>|next<sub>0</sub>|\n",
    "|---|---|---|---|---|\n",
    "| normalizer_<sub>norm</sub>|[l1]|-0.000000|NaN|[l1]|\n",
    "| bootstrap|[True]|-0.000000|NaN|[True]|\n",
    "| ccp<sub>alpha</sub>|[0.0]|-0.000000|NaN|[0.0]|\n",
    "| criterion|[mae]|-0.000000|NaN|[mae]|\n",
    "| max<sub>depth</sub>|[15, 20, 25]|1.094351|[12.92, 7.42, 12.23]|[15, 25]|\n",
    "| max<sub>features</sub>|[auto]|-0.000000|NaN|[auto]|\n",
    "| max<sub>leaf</sub><sub>nodes</sub>|[650, 700, 750]|1.097582|[14.43, 9.28, 8.86]|[650]|\n",
    "| max<sub>samples</sub>|[0.9]|-0.000000|NaN|[0.9]|\n",
    "| min<sub>impurity</sub><sub>decrease</sub>|[0.0]|-0.000000|NaN|[0.0]|\n",
    "| min<sub>impurity</sub><sub>split</sub>|[None]|-0.000000|NaN|[None]|\n",
    "| min<sub>samples</sub><sub>leaf</sub>|[1]|-0.000000|NaN|[1]|\n",
    "| min<sub>samples</sub><sub>split</sub>|[2]|-0.000000|NaN|[2]|\n",
    "| min<sub>weight</sub><sub>fraction</sub><sub>leaf</sub>|[0.0]|-0.000000|NaN|[0.0]|\n",
    "| n<sub>estimators</sub>|[50, 75, 100, 125]|1.380655|[10.9, 7.72, 7.78, 6.17]|[50]|\n",
    "| n<sub>jobs</sub>|[4]|-0.000000|NaN|[4]|\n",
    "| oob<sub>score</sub>|[True]|-0.000000|NaN|[True]|\n",
    "| random<sub>state</sub>|[None]|-0.000000|NaN|[None]|\n",
    "| verbose|[0]|-0.000000|NaN|[0]|\n",
    "| warm<sub>start</sub>|[False]|-0.000000|NaN|[False]|\n",
    "\n",
    "-   substantially limiting nodes appears to be good\n",
    "-   either strong or lax limits on depth are favored.\n",
    "-   suddenly less estimators are prefered.\n",
    "    -   n<sub>estimators</sub> is a dominant setting. It is also more conceptually clear.\n",
    "\n",
    "So, n<sub>estimators</sub> should be individually optimized at this point, in\n",
    "order to decide the tree growth limits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform a sensitivity analysis in this optimal subspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot a validation curve for n-estimators when considering unlimited trees parameters.\n",
    "\n",
    "If limiting trees on top of an \"optimal\" ensemble size improves the model, good.\n",
    "\n",
    "Adjusting the ensemble size to fit the tree optimizations is not as\n",
    "much a sensible use of the RFR architecture's strengths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.  construct subsequent HP space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [True],\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['max_squared_error'],\n",
    "     'randomforestregressor__max_depth': [None], #unlimited\n",
    "     'randomforestregressor__max_features': ['auto'],\n",
    "     'randomforestregressor__max_leaf_nodes': [None], #unlimited\n",
    "     'randomforestregressor__max_samples': [0.9],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_impurity_split': [None],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [100],\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__oob_score': [True],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the parameters settled in the grid. n estimators will be handled by the validation curve function in the next step\n",
    "cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n<sub>estimators</sub> validation scan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the 4 fold cross validation established by the LC analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VC = pvc(validation_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n",
    "         param_name='randomforestregressor__n_estimators', param_range=np.linspace(50, 150, 15).astype(int), cv=4, scoring=scorings)\n",
    "VC = VC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.FacetGrid(VC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\n",
    "p.map(sns.lineplot, 'randomforestregressor__n_estimators', \"value\")\n",
    "p.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately, it seems adjustments to ensemble size have no\n",
    "appreciable effect at these settings. this is a good indication that\n",
    "this data does not suit the RFR architecture.\n",
    "\n",
    "This visualization also helps to clarify that the current model space\n",
    "has reasonable RMSE, but nevertheless consistently permits at least one\n",
    "large outlier.\n",
    "\n",
    "There's not much point in continuing. with more minor optimizations if\n",
    "this parameter is ineffective. There is also huge variability within\n",
    "each fold. The band here is a 95% confidence interval. Trying to pick\n",
    "a estimator that narrows this band is not helpful at this point\n",
    "because sometimes the performance is actually good with the wide band,\n",
    "whereas the mean performance is catastrophically bad everywhere\n",
    "\n",
    "we're likely to see better with other models. let's wrap up and move on\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'normalizer__norm': ['l1'],\n",
    "     'randomforestregressor__bootstrap': [True],\n",
    "     'randomforestregressor__ccp_alpha': [0.0],\n",
    "     'randomforestregressor__criterion': ['mae'],\n",
    "     'randomforestregressor__max_depth': [None], #unlimited\n",
    "     'randomforestregressor__max_features': ['auto'],\n",
    "     'randomforestregressor__max_leaf_nodes': [None], #unlimited\n",
    "     'randomforestregressor__max_samples': [0.9],\n",
    "     'randomforestregressor__min_impurity_decrease': [0.0],\n",
    "     'randomforestregressor__min_impurity_split': [None],\n",
    "     'randomforestregressor__min_samples_leaf': [1],\n",
    "     'randomforestregressor__min_samples_split': [2],\n",
    "     'randomforestregressor__min_weight_fraction_leaf': [0.0],\n",
    "     'randomforestregressor__n_estimators': [100], #better performance by somome metrics, about as good as it gets\n",
    "     'randomforestregressor__n_jobs': [4],\n",
    "     'randomforestregressor__oob_score': [True],\n",
    "     'randomforestregressor__random_state': [None],\n",
    "     'randomforestregressor__verbose': [0],\n",
    "     'randomforestregressor__warm_start': [False]},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Final Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})\n",
    "cpipe.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, data = parityplot(cpipe, mc_ts, my_ts.PBE_bg_eV.to_frame(), aspect=1.0, hue=\"mix\")\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.savefig(\"./ParityPlots/rfr_c_bg.png\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(batch_score(cpipe, mc_tr, my_tr.PBE_bg_eV, **scorings)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the RFR designed here does fit the test data well. Even\n",
    "despite the possible shortcomings in the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Generality Measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The post-optimization RFR generalizes across all alloy types many\n",
    "times better than the default parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting/Importing Trained Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model for distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never load joblib/pickle files that you do not trust, they can execute\n",
    "arbitrary code on your computer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(cpipe, \"./Models/rfr_c_opt.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpipe = joblib.load(\"./Models/rfr_c_opt.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Site-Averaged Properties Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = mc.ft.derive_from(lookup, \"element\", \"Formula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BG Using Site-Averaged Properties\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Properties Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppipe = mkpipe(StandardScaler(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comp+prop model opt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mrsicms)",
   "language": "python",
   "name": "mrsicms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Linear Band Gap Models based on Perovskite Compositions\n",
    "=============================================================\n",
    "\n",
    "**Author:** Panayotis Manganaris\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"cmcl\" and \"yogi\" are our in-house modules for analyzing chemical\n",
    "compositions and enabling different nuts and bolts of ML algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurization\n",
    "import cmcl\n",
    "from cmcl import Categories\n",
    "# multi-criterion model evaluation\n",
    "from yogi.model_selection import pandas_validation_curve as pvc\n",
    "from yogi.metrics.pandas_scoring import PandasScoreAdaptor as PSA\n",
    "from yogi.metrics.pandas_scoring import batch_score\n",
    "# visualization convenience\n",
    "from spyglass.model_imaging import parityplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Intel distribution provides accelerated ml algorithms. Run this\n",
    "cell before importing the algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# feature engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, Normalizer\n",
    "# predictors\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "## pipeline workflow\n",
    "from sklearn.pipeline import make_pipeline as mkpipe\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV as gsCV\n",
    "# model eval\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, explained_variance_score, max_error\n",
    "import joblib\n",
    "#visualization\n",
    "from sklearn import set_config\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# ignore all FutureWarnings -- handling coming in a future version of yogi\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Using Cmcl and Compute Composition Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading stored targets and reference material\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = pd.read_csv(\"./mannodi_data.csv\").set_index([\"index\", \"Formula\", \"sim_cell\"])\n",
    "lookup = pd.read_csv(\"./constituent_properties.csv\").set_index(\"Formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmcl provides an \"ft\" (feature) pandas DataFrame accessor. This\n",
    "accessor exposes batch feature extraction tools. The function ft.comp\n",
    "extracts composition vectors from the formula string in a dataframe\n",
    "(or dataframe index).\n",
    "\n",
    "The abx function of the collect accessor is a convenience function for\n",
    "grouping the resulting composition constituents by site membership\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = my.ft.comp() # compute numerical compostion vectors from strings\n",
    "mc = mc.collect.abx() # convenient site groupings for perovskites data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this can be used with the logif tool in categories to quickly\n",
    "categorize records by their mix status. that status is assigned to the\n",
    "index of the respective tables for further reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixlog = mc.groupby(level=0, axis=1).count()\n",
    "mix = mixlog.pipe(Categories.logif, condition=lambda x: x>1, default=\"pure\", catstring=\"and\")\n",
    "mc = mc.assign(mix=mix).set_index(\"mix\", append=True)\n",
    "my = my.assign(mix=mix).set_index(\"mix\", append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BG Using Composition Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will be based on a simple least squares linear\n",
    "regression. The data will be normalized as discussed in visualizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Composition Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "cpipe = mkpipe(fillna, Normalizer('l1'), LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Scheme\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare subset scoring weights and ordinal group labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixweight = pd.get_dummies(mix)\n",
    "mixcat = pd.Series(OrdinalEncoder().fit_transform(mix.values.reshape(-1, 1)).reshape(-1),\n",
    "                     index=mc.index).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Scoring Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_mse = PSA(mean_squared_error).score\n",
    "scorings = {'r2': make_scorer(r2_score),\n",
    "            'ev': make_scorer(explained_variance_score),\n",
    "            'maxerr': make_scorer(max_error, greater_is_better=False),\n",
    "            'rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n",
    "            'A_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.A),\n",
    "            'B_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.B),\n",
    "            'X_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                  squared=False, sample_weight=mixweight.X),\n",
    "            'BandX_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                      squared=False, sample_weight=mixweight.BandX),\n",
    "            'Pure_rmse': make_scorer(site_mse, greater_is_better=False,\n",
    "                                     squared=False, sample_weight=mixweight.pure),}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dedicated Test Train Split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)\n",
    "train_idx, test_idx = next(sss.split(mc, mixcat)) #stratify split by mix categories\n",
    "mc_tr, mc_ts = mc.iloc[train_idx], mc.iloc[test_idx]\n",
    "my_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]\n",
    "mixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves &#x2013; Using Deterministically Random Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('multiprocessing'):\n",
    "  LC = pvc(learning_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n",
    "           train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc, scoring=scorings)\n",
    "  LC = LC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.FacetGrid(LC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\n",
    "p.map(sns.lineplot, \"train_sizes\", \"value\")\n",
    "p.add_legend()\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that 2-3 fold cross-validation is sufficient (training with\n",
    "200/400 or 260/400 points, validating with the compliment)\n",
    "\n",
    "In the case of max error, it looks like using any more than about 100 datapoints makes for a pretty good chance of overfitting, but the rest of the metrics do not support this conclusion.\n",
    "\n",
    "The Linear Regressor validation scores tend to cap out after about\n",
    "200 data points have been seen. Giving this regressor more data likely\n",
    "won't help.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Generality Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generality(estimator, X_tr, y_tr, groups_tr, X_ts, y_ts, groups_ts):\n",
    "    estimator = clone(estimator) #unfitted, cloned params\n",
    "    gentpl = gkf.split(X_tr, y_tr, groups=groups_tr), gkf.split(X_ts, y_ts, groups=groups_ts)\n",
    "    #train and test index generators, in order\n",
    "    val_scores = []\n",
    "    tst_scores = []\n",
    "    for train_idx, val_idx, _, tst_idx in [sum(gengroup, ()) for gengroup in zip(*gentpl)]:\n",
    "        tr_val_group_names = groups_tr.iloc[val_idx].index.get_level_values(\"mix\").unique()\n",
    "        ts_group_names = groups_ts.iloc[tst_idx].index.get_level_values(\"mix\").unique()\n",
    "        #fit to tr part\n",
    "        estimator.fit(X_tr.iloc[train_idx], y_tr.iloc[train_idx])\n",
    "        #get val and test scores\n",
    "        tr_val_score_series = pd.Series(batch_score(estimator, X_tr.iloc[val_idx], y_tr.iloc[val_idx], **scorings))\n",
    "        tr_val_score_series.name=\"_&_\".join(tr_val_group_names)\n",
    "        ts_score_series = pd.Series(batch_score(estimator, X_ts.iloc[tst_idx], y_ts.iloc[tst_idx], **scorings))\n",
    "        ts_score_series.name=\"_&_\".join(ts_group_names)\n",
    "        val_scores.append(tr_val_score_series)\n",
    "        tst_scores.append(ts_score_series)\n",
    "    tr_val_scores = pd.concat(val_scores, axis=1).assign(partition=\"validation\")\n",
    "    ts_scores = pd.concat(tst_scores, axis=1).assign(partition=\"test\")\n",
    "    group_scores = pd.concat([tr_val_scores, ts_scores]).round(5).drop_duplicates(keep=\"first\")\n",
    "    return group_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   extrapolations are not very accurate, but generally within 0.5 eV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default parameters used. Linear regression is very simple\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Final Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})\n",
    "cpipe.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change between tr and ts suffixes to see test vs train pairity plot\n",
    "p, data = parityplot(cpipe, mc_tr, my_tr.PBE_bg_eV.to_frame(), aspect=1.0, hue=\"mix\")\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change between tr and ts suffixes to see test vs train scores -- both are good\n",
    "pd.Series(batch_score(cpipe, mc_ts, my_ts.PBE_bg_eV, **scorings)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model is actually pretty reasonable, but it's unlikely that\n",
    "it's learning anything very fundamental about the underlying physics\n",
    "of the Pervoskite system. however, that doesn't mean it reveals nothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret = pd.Series(cpipe[-1].coef_, index=mc_tr.columns)\n",
    "interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret.groupby(level=0).aggregate(lambda x: np.sqrt(sum(x**2))) #compute the root mean square model coefficient per site feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, the coefficients that define the linear combination do\n",
    "not find much use in X-site elements. however, the B site elements\n",
    "contribute much more to the band gap on average, which is consistent\n",
    "with our physical understanding.\n",
    "\n",
    "A site elements also prove to be relevant.\n",
    "\n",
    "Likewise, with respect to the generality measure conducted earlier, it\n",
    "seems the presence of individual elements is far more predictive of\n",
    "the total band gap than mix status. This would explain why X-site and\n",
    "A-site alloys are sufficient to predict the band gaps of B-site alloys.\n",
    "Despite those groups containing no B-site alloys themselves, they do\n",
    "contain a representative sample of B-site elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BG Using Composition Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features used here, especially after l1 normalization, are\n",
    "proportions of a total. By definition they are correlated. So, it is\n",
    "worth using a more intelligent linear regression to try extracting an\n",
    "indication of the most impactful features.\n",
    "\n",
    "ElasticNet uses Lasso in combination with l1 and l2 regularization of\n",
    "the model parameters to encourage sparsity. however, it is not\n",
    "especially strict. unlike Lasso (another sparsifying linear\n",
    "regressor), ElasticNet will use multiple sets of correlated features\n",
    "instead of &#x2013; effectively randomly &#x2013; picking only one in the effort\n",
    "to return sparse coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Composition Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the alpha value, the more aggressively elasticnet will\n",
    "minimize it's weights. an alpha value of 1 will force all weights to\n",
    "zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna = SimpleImputer(strategy=\"constant\", fill_value=0.0)\n",
    "cpipe = mkpipe(fillna, Normalizer('l1'), ElasticNet(alpha=0.001, l1_ratio=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dedicated Test Train Split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=0.8, random_state=None)\n",
    "train_idx, test_idx = next(sss.split(mc, mixcat)) #stratify split by mix categories\n",
    "mc_tr, mc_ts = mc.iloc[train_idx], mc.iloc[test_idx]\n",
    "my_tr, my_ts = my.iloc[train_idx], my.iloc[test_idx]\n",
    "mixcat_tr, mixcat_ts = mixcat.iloc[train_idx], mixcat.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves &#x2013; Using Deterministically Random Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_lc = KFold(n_splits=10, shuffle=True, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('multiprocessing'):\n",
    "  LC = pvc(learning_curve, cpipe, mc_tr, my_tr.PBE_bg_eV,\n",
    "           train_sizes=np.linspace(0.1, 1.0, 10), cv=kf_lc, scoring=scorings)\n",
    "  LC = LC.melt(id_vars=[\"partition\"], ignore_index=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.FacetGrid(LC, col=\"score\", hue=\"partition\", col_wrap=3, sharey=False)\n",
    "p.map(sns.lineplot, \"train_sizes\", \"value\")\n",
    "p.add_legend()\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the normal least squares regressor, elastic net quickly saturates to a maximum score.\n",
    "\n",
    "-   scores are decent\n",
    "-   the point really is to see if any of the features are effectively redundant contributors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Generality Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generality(cpipe, mc_tr, my_tr.PBE_bg_eV, mixcat_tr, mc_ts, my_ts.PBE_bg_eV, mixcat_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   extrapolations are not very accurate, but generally within 0.5 eV\n",
    "-   scores are overall a little bit worse than the normal OLS regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default parameters used. Linear regression is very simple\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Final Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpipe = cpipe.set_params(**{k:v[0] for k,v in grid[0].items()})\n",
    "cpipe.fit(mc_tr, my_tr.PBE_bg_eV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change between tr and ts suffixes to see test vs train pairity plot\n",
    "p, data = parityplot(cpipe, mc_tr, my_tr.PBE_bg_eV.to_frame(), aspect=1.0, hue=\"mix\")\n",
    "p.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change between tr and ts suffixes to see test vs train scores -- both are good\n",
    "pd.Series(batch_score(cpipe, mc_ts, my_ts.PBE_bg_eV, **scorings)).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model is actually pretty reasonable, but it's unlikely that\n",
    "it's learning anything very fundamental about the underlying physics\n",
    "of the Pervoskite system. however, that doesn't mean it reveals nothing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret = pd.Series(cpipe[-1].coef_, index=mc_tr.columns)\n",
    "interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start to see that more complicated models appear to take\n",
    "A-site elements more seriously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret.groupby(level=0).aggregate(lambda x: np.sqrt(sum(x**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevance of A site elements seems to fluctuate depending on the random splitting -- try the splitting and final model training repeatedly to see the variation.\n",
    "\n",
    "This exercise results in one important conclusion:\n",
    "\n",
    "Despite the fact that the composition space definitely consists of\n",
    "correlated features, very few of them can be easily called\n",
    "redundant.\n",
    "\n",
    "Additionally, comparing the ElasticNet scores to the standard linear\n",
    "regression shows using all features seems to result in better\n",
    "performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mrsicms)",
   "language": "python",
   "name": "mrsicms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
